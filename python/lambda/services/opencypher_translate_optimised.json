{
  "self": {
    "lm": null,
    "traces": [],
    "train": [],
    "demos": [
      {
        "question": "```turtle\n@prefix ex: <http:\/\/example.org\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n\nex:JazzCommunity a dbo:Community ;\n    dbo:hasMember dbr:IBM_Engineering_Lifecycle_Management ;\n    dbo:hasPart ex:JazzNet ;\n    dbo:hasPart ex:JazzResources ;\n    dbo:hasPart ex:JazzBlog ;\n    dbo:hasPart ex:JazzForum .\n\nex:JazzNet a dbo:Website ;\n    dbo:description \"IBM's initiative to transform software and systems delivery by improving collaboration and productivity across the lifecycle.\" .\n\nex:JazzResources a dbo:Resource ;\n    dbo:description \"Articles, documentation and more.\" .\n\nex:JazzBlog a dbo:Blog ;\n    dbo:description \"News and technical updates impactful to IBM Engineering users.\" .\n\nex:JazzForum a dbo:Forum ;\n    dbo:description \"Connect and collaborate with IBM Engineering experts and users.\" .\n\nex:ELM_Engineering a dbo:Event ;\n    dbo:hasEvent ex:UpcomingEvents ;\n    dbo:hasParticipant dbr:IBM_Engineering_Team .\n\nex:UpcomingEvents a dbo:Event ;\n    dbo:date \"2024-10-15\"^^xsd:date ;\n    dbo:description \"Open, federated, versioned: Keys to an effective engineering digital thread, Webinar.\" .\n\nex:PerformanceImprovements a dbo:Update ;\n    dbo:relatedTo dbr:ELM_7.0.3 ;\n    dbo:description \"Performance, serviceability and other quality of service improvements in ELM 7.0.3 iFix007.\" .\n\nex:ModelBasedSystemsEngineeringSymposium a dbo:Event ;\n    dbo:location dbr:Herndon_VA ;\n    dbo:date \"2024-06-18\"^^xsd:date ;\n    dbo:description \"Join IBM Engineering at the Model Based Systems Engineering & Product Development Symposium.\" ;\n    dbo:venue \"IBM Office at 2300 Dulles Station Road \u2013 1st Floor\" .\n\nex:EngineeringTraining a dbo:Training ;\n    dbo:description \"Hands-on courses and labs from across the IBM Engineering Lifecycle Management training portfolio.\" .\n```\n---\nJazz\nWelcome to the Jazz community for IBM Engineering Lifecycle Management\nJazz.net is IBM's initiative to transform software and systems delivery by improving collaboration and productivity across the lifecycle\nStart trial\nRegister\nJazz resources\nForum\n\nConnect and collaborate with IBM Engineering experts and users\n\nLibrary\n\nArticles, documentation and more\n\nBlog\n\nNews and technical updates impactful to IBM Engineering users\n\nELM Engineering | Fri, 30 Aug 2024\nUpcoming events for our Engineering user community\n\nThe IBM Engineering team is pleased to be participating at numerous upcoming events serving the Engineering user community. Use the event link to view more details and registration information. We hope to see you at one of the following: October 2024 October 15, 2024: Open, federated, versioned: Keys to an effective engineering digital thread, Webinar...\n\nDaniel Moul | Thu, 25 Jul 2024\nPerformance, serviceability and other quality of service improvements in ELM 7.0.3 iFix007\n\nLike ELM 7.0.2 iFix018, we have delivered improvements into the 7.0.3 iFix stream so that you don\u2019t have to wait for ELM 7.1 to get the benefits. Again, we carefully selected items that are low risk and don\u2019t change practitioners\u2019 user interfaces. Unlike ELM 7.0.2 iFix018, this time we focused mostly on qualities of service....\n\nELM Engineering | Wed, 12 Jun 2024\nJoin IBM Engineering at the Model Based Systems Engineering & Product Development Symposium, Herndon, VA\n\nWhere: Herndon, Virginia (IBM Office at 2300 Dulles Station Road \u2013 1st Floor) Register at: www.ibm.biz\/herndonengineeringsymposium When: June 18, 2024 Come to IBM\u2019s Virginia office on June 18 for a full day symposium as part of our Engineering Enlightenment series. The day will consist of presentations and discussions by IBM experts and our partners Softacus...\n\nThe future belongs to those who invest in skills today. Accelerate your journey to success with hands-on courses and labs from across the IBM Engineering Lifecycle Management training portfolio.\nVisit Engineering Training\nJoin us\nLearn more\nRegister\nExplore\nStart trial",
        "response": "MERGE (jazzCommunity:Community {id: \"JazzCommunity\"})\nSET jazzCommunity.description = \"Welcome to the Jazz community for IBM Engineering Lifecycle Management\"\nMERGE (jazzNet:Website {id: \"JazzNet\"})\nSET jazzNet.description = \"IBM's initiative to transform software and systems delivery by improving collaboration and productivity across the lifecycle.\"\nMERGE (jazzResources:Resource {id: \"JazzResources\"})\nSET jazzResources.description = \"Articles, documentation and more.\"\nMERGE (jazzBlog:Blog {id: \"JazzBlog\"})\nSET jazzBlog.description = \"News and technical updates impactful to IBM Engineering users.\"\nMERGE (jazzForum:Forum {id: \"JazzForum\"})\nSET jazzForum.description = \"Connect and collaborate with IBM Engineering experts and users.\"\nMERGE (elmEngineering:Event {id: \"ELM_Engineering\"})\nMERGE (upcomingEvents:Event {id: \"UpcomingEvents\"})\nSET upcomingEvents.date = \"2024-10-15\", upcomingEvents.description = \"Open, federated, versioned: Keys to an effective engineering digital thread, Webinar.\"\nMERGE (performanceImprovements:Update {id: \"PerformanceImprovements\"})\nSET performanceImprovements.description = \"Performance, serviceability and other quality of service improvements in ELM 7.0.3 iFix007.\"\nMERGE (modelBasedSystemsEngineeringSymposium:Event {id: \"ModelBasedSystemsEngineeringSymposium\"})\nSET modelBasedSystemsEngineeringSymposium.date = \"2024-06-18\", modelBasedSystemsEngineeringSymposium.description = \"Join IBM Engineering at the Model Based Systems Engineering & Product Development Symposium.\", modelBasedSystemsEngineeringSymposium.venue = \"IBM Office at 2300 Dulles Station Road \u2013 1st Floor\"\nMERGE (engineeringTraining:Training {id: \"EngineeringTraining\"})\nSET engineeringTraining.description = \"Hands-on courses and labs from across the IBM Engineering Lifecycle Management training portfolio.\"\n\nMERGE (jazzCommunity)-[:hasMember]->(IBM_Engineering_Lifecycle_Management:IBM_Engineering_Lifecycle_Management)\nMERGE (jazzCommunity)-[:hasPart]->(jazzNet)\nMERGE (jazzCommunity)-[:hasPart]->(jazzResources)\nMERGE (jazzCommunity)-[:hasPart]->(jazzBlog)\nMERGE (jazzCommunity)-[:hasPart]->(jazzForum)\nMERGE (elmEngineering)-[:hasEvent]->(upcomingEvents)\nMERGE (elmEngineering)-[:hasParticipant]->(IBM_Engineering_Team:IBM_Engineering_Team)\nMERGE (performanceImprovements)-[:relatedTo]->(ELM_7_0_3:ELM_7_0_3)\nMERGE (modelBasedSystemsEngineeringSymposium)-[:location]->(Herndon_VA:Herndon_VA)\n\nMERGE (source:Article {id: \"5e103a13-b7e1-40b4-b42c-2410ccea2085\"})\nMERGE (jazzCommunity)-[:SOURCE_OF]->(source)\nMERGE (jazzNet)-[:SOURCE_OF]->(source)\nMERGE (jazzResources)-[:SOURCE_OF]->(source)\nMERGE (jazzBlog)-[:SOURCE_OF]->(source)\nMERGE (jazzForum)-[:SOURCE_OF]->(source)\nMERGE (elmEngineering)-[:SOURCE_OF]->(source)\nMERGE (upcomingEvents)-[:SOURCE_OF]->(source)\nMERGE (performanceImprovements)-[:SOURCE_OF]->(source)\nMERGE (modelBasedSystemsEngineeringSymposium)-[:SOURCE_OF]->(source)\nMERGE (engineeringTraining)-[:SOURCE_OF]->(source)",
        "article_id": "5e103a13-b7e1-40b4-b42c-2410ccea2085"
      },
      {
        "question": "```turtle\n@prefix dbpedia: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n@prefix rdf: <http:\/\/www.w3.org\/1999\/02\/22-rdf-syntax-ns#> .\n@prefix rdfs: <http:\/\/www.w3.org\/2000\/01\/rdf-schema#> .\n\n# Entities\ndbr:GraphSAGE a dbo:Framework ;\n    dbo:abstract \"GraphSAGE is a framework for inductive representation learning on large graphs.\"@en ;\n    dbo:feature dbr:Node_Embedding ;\n    dbo:application dbr:Machine_Learning .\n\ndbr:Node_Embedding a dbo:Concept ;\n    dbo:abstract \"Low-dimensional vector embeddings of nodes in large graphs.\"@en .\n\ndbr:Machine_Learning a dbo:Field ;\n    dbo:abstract \"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to 'learn' from data.\"@en .\n\ndbr:William_L_Hamilton a dbo:Person ;\n    dbo:contributor dbr:GraphSAGE .\n\ndbr:Rex_Ying a dbo:Person ;\n    dbo:contributor dbr:GraphSAGE .\n\ndbr:Jure_Leskovec a dbo:Person ;\n    dbo:contributor dbr:GraphSAGE .\n\n# Relations\ndbr:GraphSAGE dbo:hasContributor dbr:William_L_Hamilton ;\n    dbo:hasContributor dbr:Rex_Ying ;\n    dbo:hasContributor dbr:Jure_Leskovec ;\n    dbo:hasDataset dbr:Protein_Protein_Interactions ;\n    dbo:hasDataset dbr:Reddit .\n\ndbr:Protein_Protein_Interactions a dbo:Dataset ;\n    dbo:abstract \"A dataset of protein-protein interactions.\"@en .\n\ndbr:Reddit a dbo:Dataset ;\n    dbo:abstract \"A dataset derived from the Reddit social media platform.\"@en .\n\ndbr:GraphSAGE dbo:implementedIn dbr:TensorFlow ;\n    dbo:hasReference dbr:Inductive_Representation_Learning_on_Large_Graphs .\n```\n---\nSNAP Code\nSNAP Datasets\nWhat's new\nPapers\nLinks\nAbout\nGraphSAGE: Inductive Representation Learning on Large Graphs\n\nGraphSAGE is a framework for inductive representation learning on large graphs. GraphSAGE is used to generate low-dimensional vector representations for nodes, and is especially useful for graphs that have rich node attribute information.\n\nMotivation\nCode\nDatasets\nContributors\nReferences\nMotivation\nLow-dimensional vector embeddings of nodes in large graphs have numerous applications in machine learning (e.g., node classification, clustering, link prediction). However, most embedding frameworks are inherently transductive and can only generate embeddings for a single fixed graph. These transductive approaches do not efficiently generalize to unseen nodes (e.g., in evolving graphs), and these approaches cannot learn to generalize across different graphs. In contrast, GraphSAGE is an inductive framework that leverages node attribute information to efficiently generate representations on previously unseen data.\n\n\nTo run GraphSAGE, it needs to train on an example graph or set of graphs. After training, GraphSAGE can be used to generate node embeddings for previously unseen nodes or entirely new input graphs, as long as these graphs have the same attribute schema as the training data.\nCode\nGraphSAGE is implemented in TensorFlow and can be easily integrated into other machine learning pipelines. Code and implementation details can be found on GitHub.\nDatasets\nLinks to datasets used in the paper:\nProtein-Protein Interactions [Source] [Preprocessed]\nReddit [Source] [Preprocessed]\nPlease see the GitHub code page for details on the data format. The Web of Science citation data used in the paper can be made available to groups or individuals with valid WoS licenses.\nContributors\n\nThe following people contributed to GraphSAGE:\nWilliam L. Hamilton\nRex Ying\nJure Leskovec\n\n\nReferences\n\nInductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and J. Leskovec arXiv:1706.02216 [cs.SI], 2017.\n",
        "response": "\/\/ Create grounded entities\nMERGE (g:Framework {id: \"GraphSAGE\"})\n    SET g.abstract = \"GraphSAGE is a framework for inductive representation learning on large graphs.\"\nMERGE (n:Concept {id: \"Node_Embedding\"})\n    SET n.abstract = \"Low-dimensional vector embeddings of nodes in large graphs.\"\nMERGE (m:Field {id: \"Machine_Learning\"})\n    SET m.abstract = \"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to 'learn' from data.\"\nMERGE (w:Person {id: \"William_L_Hamilton\"})\nMERGE (r:Person {id: \"Rex_Ying\"})\nMERGE (j:Person {id: \"Jure_Leskovec\"})\nMERGE (p:Dataset {id: \"Protein_Protein_Interactions\"})\n    SET p.abstract = \"A dataset of protein-protein interactions.\"\nMERGE (d:Dataset {id: \"Reddit\"})\n    SET d.abstract = \"A dataset derived from the Reddit social media platform.\"\nMERGE (t:Framework {id: \"TensorFlow\"})\nMERGE (ref:Reference {id: \"Inductive_Representation_Learning_on_Large_Graphs\"})\n\n\/\/ Create grounded relations\nMERGE (g)-[:hasContributor]->(w)\nMERGE (g)-[:hasContributor]->(r)\nMERGE (g)-[:hasContributor]->(j)\nMERGE (g)-[:hasDataset]->(p)\nMERGE (g)-[:hasDataset]->(d)\nMERGE (g)-[:implementedIn]->(t)\nMERGE (g)-[:hasReference]->(ref)\n\n\/\/ Add SOURCE_OF relations\nWITH g, n, m, w, r, j, p, d, t, ref\nCREATE (a:Article {id: \"1a4c3ec8-4102-40e2-af28-62ce657e3e25\"})-[:SOURCE_OF]->(g)\nCREATE (a)-[:SOURCE_OF]->(n)\nCREATE (a)-[:SOURCE_OF]->(m)\nCREATE (a)-[:SOURCE_OF]->(w)\nCREATE (a)-[:SOURCE_OF]->(r)\nCREATE (a)-[:SOURCE_OF]->(j)\nCREATE (a)-[:SOURCE_OF]->(p)\nCREATE (a)-[:SOURCE_OF]->(d)\nCREATE (a)-[:SOURCE_OF]->(t)\nCREATE (a)-[:SOURCE_OF]->(ref)",
        "article_id": "1a4c3ec8-4102-40e2-af28-62ce657e3e25"
      },
      {
        "question": "```turtle\n@prefix dbpedia: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dct: <http:\/\/purl.org\/dc\/terms\/> .\n\n# Entities\ndbr:GreenKGC a dbo:Software ;\n    dct:title \"GreenKGC: A Lightweight Knowledge Graph Completion Method\" ;\n    dbo:abstract \"A lightweight modularized KGC solution proposed to address the issue of high-dimensional embedding space in knowledge graph completion.\" ;\n    dbo:creator dbr:Yun-Cheng_Wang, dbr:Xiou_Ge, dbr:Bin_Wang, dbr:C.-C._Jay_Kuo ;\n    dbo:subject dbr:Artificial_Intelligence ;\n    dct:issued \"2022-08-19\"^^xsd:date ;\n    dct:modified \"2023-07-09\"^^xsd:date ;\n    dbo:related dbr:Knowledge_graph_completion .\n\ndbr:Yun-Cheng_Wang a dbo:Person ;\n    dct:contributor dbr:GreenKGC .\n\ndbr:Xiou_Ge a dbo:Person ;\n    dct:contributor dbr:GreenKGC .\n\ndbr:Bin_Wang a dbo:Person ;\n    dct:contributor dbr:GreenKGC .\n\ndbr:C.-C._Jay_Kuo a dbo:Person ;\n    dct:contributor dbr:GreenKGC .\n\n# Relations\ndbr:GreenKGC dbo:hasModule dbr:Representation_learning, dbr:Feature_pruning, dbr:Decision_learning .\n\ndbr:Knowledge_graph_completion a dbo:Process ;\n    dbo:related dbr:GreenKGC .\n\ndbr:GreenKGC dbo:outperforms dbr:SOTA_methods ;\n    dbo:inDataset dbr:Most_datasets .\n\ndbr:GreenKGC dbo:achievesPerformance dbr:Low_dimensions, dbr:High_dimensions .\n```\n---\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\nDonate\n>\ncs\n>\narXiv:2208.09137\n\nHelp | Advanced Search\n\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nComputer Science > Artificial Intelligence\n[Submitted on 19 Aug 2022 (v1), last revised 9 Jul 2023 (this version, v2)]\nGreenKGC: A Lightweight Knowledge Graph Completion Method\nYun-Cheng Wang, Xiou Ge, Bin Wang, C.-C. Jay Kuo\nKnowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple scoring function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to a larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile\/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance against high-dimensional models with a much smaller model size.\nComments:\tAccepted to ACL2023\nSubjects:\tArtificial Intelligence (cs.AI)\nCite as:\tarXiv:2208.09137 [cs.AI]\n\u00a0\t(or arXiv:2208.09137v2 [cs.AI] for this version)\n\u00a0\t\nhttps:\/\/doi.org\/10.48550\/arXiv.2208.09137\nFocus to learn more\nSubmission history\nFrom: Yun-Cheng Wang [view email]\n[v1] Fri, 19 Aug 2022 03:33:45 UTC (1,588 KB)\n[v2] Sun, 9 Jul 2023 09:34:39 UTC (3,797 KB)\n\nAccess Paper:\nView PDF\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev \u00a0 | \u00a0 next\u00a0>\n\nnew | recent | 2022-08\nChange to browse by:\ncs\n\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nExport BibTeX Citation\nBookmark\n \nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nDemos\nRelated Papers\nAbout arXivLabs\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\nAbout\nHelp\nContact\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\n\narXiv Operational Status \nGet status notifications via email or slack",
        "response": "\/\/ Create grounded entities\nMERGE (g:Software {id: \"dbr:GreenKGC\"})\n    SET g.title = \"GreenKGC: A Lightweight Knowledge Graph Completion Method\",\n        g.abstract = \"A lightweight modularized KGC solution proposed to address the issue of high-dimensional embedding space in knowledge graph completion.\",\n        g.issued = date(\"2022-08-19\"),\n        g.modified = date(\"2023-07-09\")\n\nMERGE (y:Person {id: \"dbr:Yun-Cheng_Wang\"})\nMERGE (x:Person {id: \"dbr:Xiou_Ge\"})\nMERGE (b:Person {id: \"dbr:Bin_Wang\"})\nMERGE (j:Person {id: \"dbr:C.-C._Jay_Kuo\"})\n\n\/\/ Create relations\nMERGE (g)-[:CREATOR]->(y)\nMERGE (g)-[:CREATOR]->(x)\nMERGE (g)-[:CREATOR]->(b)\nMERGE (g)-[:CREATOR]->(j)\n\nMERGE (g)-[:HAS_MODULE]->(r:Module {id: \"dbr:Representation_learning\"})\nMERGE (g)-[:HAS_MODULE]->(f:Module {id: \"dbr:Feature_pruning\"})\nMERGE (g)-[:HAS_MODULE]->(d:Module {id: \"dbr:Decision_learning\"})\n\nMERGE (k:Process {id: \"dbr:Knowledge_graph_completion\"})\nMERGE (g)-[:RELATED]->(k)\nMERGE (k)-[:RELATED]->(g)\n\nMERGE (g)-[:OUTPERFORMS]->(s:SOTA_methods)\nMERGE (g)-[:IN_DATASET]->(m:Most_datasets)\n\nMERGE (g)-[:ACHIEVES_PERFORMANCE]->(l:Low_dimensions)\nMERGE (g)-[:ACHIEVES_PERFORMANCE]->(h:High_dimensions)\n\n\/\/ Add SOURCE_OF relations\nWITH g, y, x, b, j, r, f, d, k, s, m, l, h \/\/ Include all relevant nodes in the WITH clause\nMATCH (e)\nWHERE e.id IN [\"dbr:GreenKGC\", \"dbr:Yun-Cheng_Wang\", \"dbr:Xiou_Ge\", \"dbr:Bin_Wang\", \"dbr:C.-C._Jay_Kuo\", \"dbr:Representation_learning\", \"dbr:Feature_pruning\", \"dbr:Decision_learning\", \"dbr:Knowledge_graph_completion\", \"dbr:SOTA_methods\", \"dbr:Most_datasets\", \"dbr:Low_dimensions\", \"dbr:High_dimensions\"]\nMERGE (a:Article {id: \"cc265fa3-eefe-4764-8ab7-45fc5063e34c\"})-[:SOURCE_OF]->(e)",
        "article_id": "cc265fa3-eefe-4764-8ab7-45fc5063e34c"
      },
      {
        "question": "```turtle\n@prefix dgl: <http:\/\/dbpedia.org\/resource\/DGL> .\n@prefix ex: <http:\/\/dbpedia.org\/resource\/Example> .\n@prefix paper: <http:\/\/dbpedia.org\/resource\/Paper> .\n@prefix code: <http:\/\/dbpedia.org\/resource\/Code> .\n@prefix tag: <http:\/\/dbpedia.org\/resource\/Tag> .\n@prefix rdf: <http:\/\/www.w3.org\/1999\/02\/22-rdf-syntax-ns#> .\n@prefix owl: <http:\/\/www.w3.org\/2002\/07\/owl#> .\n\ndgl:Official_DGL_Examples a dgl:Repository ;\n    dgl:containsExample ex:Example1, ex:Example2, ex:Example3, ex:Example4, ex:Example5, ex:Example6, ex:Example7, ex:Example8, ex:Example9, ex:Example10, ex:Example11, ex:Example12, ex:Example13, ex:Example14, ex:Example15, ex:Example16, ex:Example17, ex:Example18, ex:Example19, ex:Example20, ex:Example21, ex:Example22, ex:Example23, ex:Example24, ex:Example25, ex:Example26, ex:Example27, ex:Example28, ex:Example29, ex:Example30, ex:Example31, ex:Example32, ex:Example33, ex:Example34, ex:Example35, ex:Example36, ex:Example37, ex:Example38, ex:Example39, ex:Example40, ex:Example41, ex:Example42, ex:Example43, ex:Example44, ex:Example45, ex:Example46, ex:Example47, ex:Example48, ex:Example49, ex:Example50 .\n\nex:Example1 a ex:Example ;\n    ex:hasPaper paper:ARGO ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:semi_supervised_node_classification .\n\nex:Example2 a ex:Example ;\n    ex:hasPaper paper:From_Cluster_Assumption_to_Graph_Convolution ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:semi_supervised_node_classification .\n\nex:Example3 a ex:Example ;\n    ex:hasPaper paper:Layer_Neighbor_Sampling ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:node_classification, tag:weighted_graphs, tag:sampling .\n\nex:Example4 a ex:Example ;\n    ex:hasPaper paper:Learning_Protein_and_Small_Molecule_binding_sites ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:semi_supervised_node_classification .\n\nex:Example5 a ex:Example ;\n    ex:hasPaper paper:Learning_Hierarchical_Graph_Neural_Networks ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:clustering .\n\nex:Example6 a ex:Example ;\n    ex:hasPaper paper:Boost_then_Convolve ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:semi_supervised_node_classification, tag:tabular_data, tag:GBDT .\n\nex:Example7 a ex:Example ;\n    ex:hasPaper paper:Combining_Label_Propagation ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:efficiency, tag:node_classification, tag:label_propagation .\n\nex:Example8 a ex:Example ;\n    ex:hasPaper paper:Point_Transformer ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:point_cloud_classification, tag:point_cloud_part_segmentation .\n\nex:Example9 a ex:Example ;\n    ex:hasPaper paper:PCT ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:point_cloud_classification, tag:point_cloud_part_segmentation .\n\nex:Example10 a ex:Example ;\n    ex:hasPaper paper:How_Attentive_are_Graph_Attention_Networks ;\n    ex:hasCode code:PyTorch ;\n    ex:hasTag tag:graph_attention, tag:gat, tag:gatv2, tag:attention .\n\n# Additional examples would follow the same structure...\n```\nThis Turtle RDF format captures the entities and relationships described in the text, using DBpedia ontologies to represent the DGL repository, examples, papers, code, and tags. Each example is linked to its respective paper, code, and tags.\n---\nSkip to content\nNavigation Menu\ndmlc\n\/\ndgl\nType \/ to search\nCode\nIssues\n409\nPull requests\n119\nActions\nProjects\n1\nSecurity\nInsights\nFiles\n\u00a0master\nt\nREADME.md\nBreadcrumbs\ndgl\/examples\n\/README.md\nLatest commit\n\u00a0\nHistory\nHistory\nFile metadata and controls\nPreview\nCode\nBlame\n431 lines (362 loc) \u00b7 35.5 KB\nRaw\nOfficial DGL Examples and Modules\n\nThe folder contains example implementations of selected research papers related to Graph Neural Networks. Note that the examples may not work with incompatible DGL versions.\n\nFor examples working with the latest master (or the latest nightly build), check out https:\/\/github.com\/dmlc\/dgl\/tree\/master\/examples.\nFor examples working with a certain release, check out https:\/\/github.com\/dmlc\/dgl\/tree\/<release_version>\/examples (E.g., https:\/\/github.com\/dmlc\/dgl\/tree\/0.5.x\/examples)\n\nTo quickly locate the examples of your interest, search for the tagged keywords or use the search tool on dgl.ai.\n\n2024\nLin et al. ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor. Paper link\n\nExample code: PyTorch\n\nTags: semi-supervised node classification\n\n2023\nZheng Wang et al. From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited. Paper link\n\nExample code: PyTorch\n\nTags: semi-supervised node classification\n\n2022\nBalin et al. Layer-Neighbor Sampling -- Defusing Neighborhood Explosion in GNNs. Paper link\nExample code: PyTorch\nTags: node classification, weighted graphs, sampling\n2021\nMallet et al. Learning Protein and Small Molecule binding sites in RNA molecules with 2.5D graphs. Paper link\nExample code: PyTorch\nTags: semi-supervised node classification\nXing et al. Learning Hierarchical Graph Neural Networks for Image Clustering.\nExample code: PyTorch\nTags: clustering\nIvanov et al. Boost then Convolve: Gradient Boosting Meets Graph Neural Networks. Paper link.\nExample code: PyTorch\nTags: semi-supervised node classification, tabular data, GBDT\nHuang et al. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks. Paper link.\nExample code: PyTorch\nTags: efficiency, node classification, label propagation\nZhao et al. Point Transformer. Paper link.\nExample code: PyTorch\nTags: point cloud classification, point cloud part-segmentation\nGuo et al. PCT: Point cloud transformer. Paper link.\nExample code: PyTorch\nTags: point cloud classification, point cloud part-segmentation\nBrody et al. How Attentive are Graph Attention Networks? Paper link.\nExample code: PyTorch\nTags: graph attention, gat, gatv2, attention\nThakoor et al. Large-Scale Representation Learning on Graphs via Bootstrapping. Paper link.\nExample code: PyTorch\nTags: contrastive learning for node classification.\nBouritsas et al. Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. Paper link.\nExample code: PyTorch\nTags: subgraph isomorphism counting, graph classification.\nSong et al. Network In Graph Neural Network. Paper link.\nExample code: PyTorch\nTags: model-agnostic methodology, link prediction, open graph benchmark.\nQin et al. BiPointNet: Binary Neural Network for Point Clouds. Paper link\nExample code: PyTorch\nTags: point cloud classification, network binarization.\n2020\n\nWagh et al. EEG-GCNN: Augmenting Electroencephalogram-based Neurological Disease Diagnosis using a Domain-guided Graph Convolutional Neural Network. Paper link.\n\nExample code: PyTorch\nTags: graph classification, eeg representation learning, brain activity, graph convolution, neurological disease classification, large dataset, edge weights, node features, fully-connected graph, graph neural network\n\nWang et al. Network Embedding with Completely-imbalanced Labels. Paper link.\n\nExample code: PyTorch\nTags: node classification, network embedding, completely-imbalanced labels\n\nHassani and Khasahmadi. Contrastive Multi-View Representation Learning on Graphs. Paper link.\n\nExample code: PyTorch\nTags: graph diffusion, self-supervised learning\n\nZhu et al. Deep Graph Contrastive Representation Learning. Paper link.\n\nExample code: PyTorch\nTags: contrastive learning for node classification.\n\nFeng et al. Graph Random Neural Network for Semi-Supervised Learning on Graphs. Paper link.\n\nExample code: PyTorch\nTags: semi-supervised node classification, simplifying graph convolution, data augmentation\n\nHu et al. Heterogeneous Graph Transformer. Paper link.\n\nExample code: PyTorch\nTags: dynamic heterogeneous graph, large-scale, node classification, link prediction\n\nChen. Graph Convolutional Networks for Graphs with Multi-Dimensionally Weighted Edges. Paper link.\n\nExample code: PyTorch on ogbn-proteins\nTags: node classification, weighted graphs, OGB\n\nFrasca et al. SIGN: Scalable Inception Graph Neural Networks. Paper link.\n\nExample code: PyTorch on ogbn-arxiv\/products\/mag, PyTorch\nTags: node classification, OGB, large-scale, heterogeneous graph\n\nHu et al. Strategies for Pre-training Graph Neural Networks. Paper link.\n\nExample code: Molecule embedding, PyTorch for custom data\nTags",
        "response": "\/\/ Create grounded entities and relations based on the provided Turtle RDF and the text\nMERGE (dgl:Repository {name: \"Official DGL Examples\"})\nMERGE (ex1:Example {name: \"Example1\"})\nMERGE (ex2:Example {name: \"Example2\"})\nMERGE (ex3:Example {name: \"Example3\"})\nMERGE (ex4:Example {name: \"Example4\"})\nMERGE (ex5:Example {name: \"Example5\"})\nMERGE (ex6:Example {name: \"Example6\"})\nMERGE (ex7:Example {name: \"Example7\"})\nMERGE (ex8:Example {name: \"Example8\"})\nMERGE (ex9:Example {name: \"Example9\"})\nMERGE (ex10:Example {name: \"Example10\"})\n\n\/\/ Create grounded papers\nMERGE (paperARGO:Paper {name: \"ARGO\"})\nMERGE (paperCluster:Paper {name: \"From Cluster Assumption to Graph Convolution\"})\nMERGE (paperLayer:Paper {name: \"Layer Neighbor Sampling\"})\nMERGE (paperProtein:Paper {name: \"Learning Protein and Small Molecule binding sites\"})\nMERGE (paperHierarchical:Paper {name: \"Learning Hierarchical Graph Neural Networks\"})\nMERGE (paperBoost:Paper {name: \"Boost then Convolve\"})\nMERGE (paperCombining:Paper {name: \"Combining Label Propagation\"})\nMERGE (paperPointTransformer:Paper {name: \"Point Transformer\"})\nMERGE (paperPCT:Paper {name: \"PCT\"})\nMERGE (paperAttention:Paper {name: \"How Attentive are Graph Attention Networks\"})\n\n\/\/ Create grounded codes\nMERGE (codePyTorch:Code {name: \"PyTorch\"})\n\n\/\/ Create grounded tags\nMERGE (tagSemiSupervised:Tag {name: \"semi_supervised_node_classification\"})\nMERGE (tagNodeClassification:Tag {name: \"node_classification\"})\nMERGE (tagWeightedGraphs:Tag {name: \"weighted_graphs\"})\nMERGE (tagClustering:Tag {name: \"clustering\"})\nMERGE (tagEfficiency:Tag {name: \"efficiency\"})\nMERGE (tagLabelPropagation:Tag {name: \"label_propagation\"})\nMERGE (tagPointCloudClassification:Tag {name: \"point_cloud_classification\"})\nMERGE (tagPointCloudPartSegmentation:Tag {name: \"point_cloud_part_segmentation\"})\nMERGE (tagGraphAttention:Tag {name: \"graph_attention\"})\nMERGE (tagGAT:Tag {name: \"gat\"})\nMERGE (tagGATv2:Tag {name: \"gatv2\"})\nMERGE (tagAttention:Tag {name: \"attention\"})\n\n\/\/ Create relationships\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex1)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex2)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex3)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex4)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex5)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex6)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex7)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex8)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex9)\nMERGE (dgl)-[:CONTAINS_EXAMPLE]->(ex10)\n\nMERGE (ex1)-[:HAS_PAPER]->(paperARGO)\nMERGE (ex1)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex1)-[:HAS_TAG]->(tagSemiSupervised)\n\nMERGE (ex2)-[:HAS_PAPER]->(paperCluster)\nMERGE (ex2)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex2)-[:HAS_TAG]->(tagSemiSupervised)\n\nMERGE (ex3)-[:HAS_PAPER]->(paperLayer)\nMERGE (ex3)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex3)-[:HAS_TAG]->(tagNodeClassification)\nMERGE (ex3)-[:HAS_TAG]->(tagWeightedGraphs)\n\nMERGE (ex4)-[:HAS_PAPER]->(paperProtein)\nMERGE (ex4)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex4)-[:HAS_TAG]->(tagSemiSupervised)\n\nMERGE (ex5)-[:HAS_PAPER]->(paperHierarchical)\nMERGE (ex5)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex5)-[:HAS_TAG]->(tagClustering)\n\nMERGE (ex6)-[:HAS_PAPER]->(paperBoost)\nMERGE (ex6)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex6)-[:HAS_TAG]->(tagSemiSupervised)\nMERGE (ex6)-[:HAS_TAG]->(tagGBDT)\n\nMERGE (ex7)-[:HAS_PAPER]->(paperCombining)\nMERGE (ex7)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex7)-[:HAS_TAG]->(tagEfficiency)\nMERGE (ex7)-[:HAS_TAG]->(tagNodeClassification)\nMERGE (ex7)-[:HAS_TAG]->(tagLabelPropagation)\n\nMERGE (ex8)-[:HAS_PAPER]->(paperPointTransformer)\nMERGE (ex8)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex8)-[:HAS_TAG]->(tagPointCloudClassification)\nMERGE (ex8)-[:HAS_TAG]->(tagPointCloudPartSegmentation)\n\nMERGE (ex9)-[:HAS_PAPER]->(paperPCT)\nMERGE (ex9)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex9)-[:HAS_TAG]->(tagPointCloudClassification)\nMERGE (ex9)-[:HAS_TAG]->(tagPointCloudPartSegmentation)\n\nMERGE (ex10)-[:HAS_PAPER]->(paperAttention)\nMERGE (ex10)-[:HAS_CODE]->(codePyTorch)\nMERGE (ex10)-[:HAS_TAG]->(tagGraphAttention)\nMERGE (ex10)-[:HAS_TAG]->(tagGAT)\nMERGE (ex10)-[:HAS_TAG]->(tagGATv2)\nMERGE (ex10)-[:HAS_TAG]->(tagAttention)\n\n\/\/ Add SOURCE_OF relations\nWITH dgl, ex1, ex2, ex3, ex4, ex5, ex6, ex7, ex8, ex9, ex10, paperARGO, paperCluster, paperLayer, paperProtein, paperHierarchical, paperBoost, paperCombining, paperPointTransformer, paperPCT, paperAttention, codePyTorch, tagSemiSupervised, tagNodeClassification, tagWeightedGraphs, tagClustering, tagEfficiency, tagLabelPropagation, tagPointCloudClassification, tagPointCloudPartSegmentation, tagGraphAttention, tagGAT, tagGATv2, tagAttention\nMATCH (entity)\nWHERE entity.name IN [\"Official DGL Examples\", \"Example1\", \"Example2\", \"Example3\", \"Example4\", \"Example5\", \"Example6\", \"Example7\", \"Example8\", \"Example9\", \"Example10\", \"ARGO\", \"From Cluster Assumption to Graph Convolution\", \"Layer Neighbor Sampling\", \"Learning Protein and Small Molecule binding sites\", \"Learning Hierarchical Graph Neural Networks\", \"Boost then Convolve\", \"Combining Label Propagation\", \"Point Transformer\", \"PCT\", \"How Attentive are Graph Attention Networks\", \"PyTorch\", \"semi_supervised_node_classification\", \"node_classification\", \"weighted_graphs\", \"clustering\", \"efficiency\", \"label_propagation\", \"point_cloud_classification\", \"point_cloud_part_segmentation\", \"graph_attention\", \"gat\", \"gatv2\", \"attention\"]\nMERGE (source:Article {id: \"8328f937-bfbe-425e-9443-8bfb890ea10b\"})\nMERGE (source)-[:SOURCE_OF]->(entity)",
        "article_id": "8328f937-bfbe-425e-9443-8bfb890ea10b"
      },
      {
        "question": "```turtle\n@prefix aws: <http:\/\/dbpedia.org\/resource\/Amazon_Web_Services> .\n@prefix model: <http:\/\/dbpedia.org\/resource\/Amazon_Titan> .\n@prefix product: <http:\/\/dbpedia.org\/resource\/Amazon_Bedrock> .\n@prefix type: <http:\/\/dbpedia.org\/ontology\/Product> .\n@prefix feature: <http:\/\/dbpedia.org\/ontology\/Feature> .\n@prefix usecase: <http:\/\/dbpedia.org\/ontology\/UseCase> .\n@prefix lang: <http:\/\/dbpedia.org\/ontology\/Language> .\n\naws:Amazon_Web_Services a type:Company ;\n    type:offers product:Amazon_Bedrock .\n\nproduct:Amazon_Bedrock a type:Product ;\n    feature:hasModel model:Amazon_Titan .\n\nmodel:Amazon_Titan a type:Model ;\n    feature:hasFeature \"High-performing foundation models\" ;\n    feature:hasFeature \"Built-in support for responsible AI\" ;\n    feature:hasFeature \"Easy customization\" ;\n    usecase:supportsUseCase usecase:TextGeneration, usecase:ImageGeneration, usecase:SemanticSearch .\n\nmodel:Amazon_Titan_Text_Premier a type:Model ;\n    feature:hasFeature \"Large language model\" ;\n    lang:supportsLanguage lang:English ;\n    usecase:supportsUseCase usecase:Chat, usecase:Summarization, usecase:CodeGeneration .\n\nmodel:Amazon_Titan_Text_Express a type:Model ;\n    feature:hasFeature \"Balance of price and performance\" ;\n    lang:supportsLanguage lang:English ;\n    usecase:supportsUseCase usecase:OpenEndedTextGeneration, usecase:Summarization .\n\nmodel:Amazon_Titan_Text_Lite a type:Model ;\n    feature:hasFeature \"Cost-effective and highly customizable\" ;\n    lang:supportsLanguage lang:English ;\n    usecase:supportsUseCase usecase:Summarization, usecase:Copywriting .\n\nmodel:Amazon_Titan_Text_Embeddings a type:Model ;\n    feature:hasFeature \"Translates text into vector representations\" ;\n    lang:supportsLanguage lang:English ;\n    usecase:supportsUseCase usecase:TextRetrieval, usecase:SemanticSimilarity .\n\nmodel:Amazon_Titan_Image_Generator a type:Model ;\n    feature:hasFeature \"Generate realistic images using text prompts\" ;\n    lang:supportsLanguage lang:English ;\n    usecase:supportsUseCase usecase:TextToImageGeneration, usecase:ImageEditing .\n\nmodel:Amazon_Titan_Image_Generator_v2 a type:Model ;\n    feature:hasFeature \"Generate photorealistic images\" ;\n    lang:supportsLanguage lang:English ;\n    usecase:supportsUseCase usecase:TextToImageGeneration, usecase:ImageEditing .\n```\n---\nSkip to main content\nClick here to return to Amazon Web Services homepage\nAbout AWS\nContact Us\nSupport \u00a0 \nEnglish \u00a0 \nMy Account \u00a0 \nSign In to the Console\n \nAmazon Q Products Solutions Pricing Documentation Learn Partner Network AWS Marketplace Customer Enablement Events Explore More\nAmazon Bedrock\nOverview\nFeatures\u00a0 \nPricing\nModel Providers\u00a0 \nFAQs\nTestimonials\nResources\n\nAWS re:Invent |\u00a0Browse the 2024 session catalog to explore all the opportunities to learn, innovate, and connect \u00bb\n\nGenerative AI\u203a Amazon Bedrock\u203a Titan\nAmazon Titan in Amazon Bedrock\nHigh-performing foundation models from Amazon\nGet started with Amazon Titan in Amazon Bedrock\nBenefits\nBroad range of applications\nDeliver relevant search results\nBuilt-in support for responsible AI\nEasy customization\nMeet Amazon Titan\n\nExclusive to Amazon Bedrock, the Amazon Titan family of models incorporates Amazon\u2019s 25 years of experience innovating with AI and machine learning across its business. Amazon Titan foundation models (FMs) provide customers with a breadth of high-performing image, multimodal, and text model choices, via a fully managed API. Amazon Titan models are created by AWS and pretrained on large datasets, making them powerful, general-purpose models built to support a variety of use cases, while also supporting the responsible use of AI. Use them as is or privately customize them with your own data.\n\nUse cases\nText generation\nSummarization\nSemantic search\nImage generation\nRetrieval Augmented Generation (RAG)\nModel versions\nAmazon Titan Text Premier\n\nAmazon Titan Text Premier is a powerful and advanced large language model (LLM) within the Amazon Titan Text family, designed to deliver superior performance across a wide range of enterprise applications. This model is optimized for integration with Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases, making it an ideal option for building interactive generative AI applications that can use your APIs and interact with your data.\u00a0\n\n\nMax tokens:\u00a032K\n\nLanguages:\u00a0English\n\nFine-tuning supported: Yes (preview)\n\nSupported use cases:\u00a0Chat, chain of thought reasoning, open-ended text generation, brainstorming, summarization, code generation, table creation, data formatting, paraphrasing, rewriting, extraction, and Q&A.\n\nRead the blog\n\nWatch Titan Text Premier demo\n\nAmazon Titan Text Express\n\nLLM offering a balance of price and performance.\n\n\nMax tokens: 8K\n\nLanguages:\u00a0English (GA), 100+ languages available (preview)\n\nFine-tuning supported: Yes\n\nSupported use cases:\u00a0Open-ended text generation, brainstorming, summarization, code generation, table creation, data formatting, paraphrasing, chain of thought, rewriting, extraction, Q&A, and chat.\n\nWatch Amazon Titan Text demo\n\nWatch re:Invent session\n\nAmazon Titan Text Lite\n\nCost-effective and highly customizable LLM. Right-sized for specific use cases, ideal for\u00a0text generation tasks and fine-tuning.\n\n\nMax tokens: 4K\n\nLanguages: English\n\nFine-tuning supported: yes\n\nSupported use cases:\u00a0Summarization and copywriting.\n\nWatch Amazon Titan Text demo\n\nWatch re:Invent session\n\n\nAmazon Titan Text Embeddings\n\nModel that translates text into vector representations (embeddings).\n\n\nMax tokens: 8K\n\nLanguages: 25+ languages\n\nFine-tuning supported: no\n\n\nEmbeddings: 1,536\n\nSupported use cases:\u00a0Text retrieval, semantic similarity, and clustering.\n\nRead the blog\n\nAmazon Titan Text Embeddings V2\n\nEmbeddings model optimized for high accuracy and retrieval performance at smaller dimensions for reduced storage and latency.\n\n\nMax tokens: 8K\n\nLanguages:\u00a0100+ in pretraining\n\nFine-tuning supported: no\n\nNormalization supported:\u00a0yes\n\nEmbeddings:\u00a0256, 512, 1024\n\nSupported use cases: Semantic similarity searching to find documents (for example, detecting plagiarism), classifying labels into data-based learned representations (for example, categorizing movies into genres), and improving quality and relevance of retrieved or generated search results.\n\nRead the blog\n\nRead the blog\n\nAmazon Titan Multimodal Embeddings\n\nPowers accurate multimodal search and recommendation experiences.\n\n\nMax tokens:\u00a0128\n\nMax images size: 25 MB\n\nLanguages: English\n\nFine-tuning supported: Yes\n\n\nEmbeddings: 1,024 (default), 384, 256\n\n\nSupported use cases:\u00a0Search, recommendation, and personalization.\n\n\nAmazon Titan Image Generator\n\nGenerate realistic, studio-quality images using text prompts.\n\nMax characters: 512\n\nLanguages:\u00a0English\n\n\nFine-tuning supported: yes\n\nSupported use cases: Text-to-image generation, image editing, and image variations.\n\nRead the blog\n\nWatch Amazon Titan Image Generator demo\n\nWatch watermark detection demo\n\nWatch re:Invent session\n\nAmazon Titan Image Generator v2\n\nGenerate photorealistic images, with support for image conditioning, subject consistency, instant customization and background removal\n\n\nMax characters: 512\n\nSupported input\/output image formats: PNG, JPEG\n\nLanguages: English\n\nFine-tuning supported: yes\n\nSupported use-cases: Text-to-image generation, image editing, image v",
        "response": "MERGE (aws:Company {id: \"Amazon_Web_Services\"})\nMERGE (product:Product {id: \"Amazon_Bedrock\"})\nMERGE (model:Model {id: \"Amazon_Titan\"})\nMERGE (model_premier:Model {id: \"Amazon_Titan_Text_Premier\"})\nMERGE (model_express:Model {id: \"Amazon_Titan_Text_Express\"})\nMERGE (model_lite:Model {id: \"Amazon_Titan_Text_Lite\"})\nMERGE (model_embeddings:Model {id: \"Amazon_Titan_Text_Embeddings\"})\nMERGE (model_image_gen:Model {id: \"Amazon_Titan_Image_Generator\"})\nMERGE (model_image_gen_v2:Model {id: \"Amazon_Titan_Image_Generator_v2\"})\n\nMERGE (aws)-[:OFFERS]->(product)\nMERGE (product)-[:HAS_MODEL]->(model)\nMERGE (model)-[:HAS_FEATURE]->(:Feature {name: \"High-performing foundation models\"})\nMERGE (model)-[:HAS_FEATURE]->(:Feature {name: \"Built-in support for responsible AI\"})\nMERGE (model)-[:HAS_FEATURE]->(:Feature {name: \"Easy customization\"})\nMERGE (model)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"TextGeneration\"})\nMERGE (model)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"ImageGeneration\"})\nMERGE (model)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"SemanticSearch\"})\n\nMERGE (model_premier)-[:HAS_FEATURE]->(:Feature {name: \"Large language model\"})\nMERGE (model_premier)-[:SUPPORTS_LANGUAGE]->(:Language {name: \"English\"})\nMERGE (model_premier)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"Chat\"})\nMERGE (model_premier)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"Summarization\"})\nMERGE (model_premier)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"CodeGeneration\"})\n\nMERGE (model_express)-[:HAS_FEATURE]->(:Feature {name: \"Balance of price and performance\"})\nMERGE (model_express)-[:SUPPORTS_LANGUAGE]->(:Language {name: \"English\"})\nMERGE (model_express)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"OpenEndedTextGeneration\"})\nMERGE (model_express)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"Summarization\"})\n\nMERGE (model_lite)-[:HAS_FEATURE]->(:Feature {name: \"Cost-effective and highly customizable\"})\nMERGE (model_lite)-[:SUPPORTS_LANGUAGE]->(:Language {name: \"English\"})\nMERGE (model_lite)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"Summarization\"})\nMERGE (model_lite)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"Copywriting\"})\n\nMERGE (model_embeddings)-[:HAS_FEATURE]->(:Feature {name: \"Translates text into vector representations\"})\nMERGE (model_embeddings)-[:SUPPORTS_LANGUAGE]->(:Language {name: \"English\"})\nMERGE (model_embeddings)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"TextRetrieval\"})\nMERGE (model_embeddings)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"SemanticSimilarity\"})\n\nMERGE (model_image_gen)-[:HAS_FEATURE]->(:Feature {name: \"Generate realistic images using text prompts\"})\nMERGE (model_image_gen)-[:SUPPORTS_LANGUAGE]->(:Language {name: \"English\"})\nMERGE (model_image_gen)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"TextToImageGeneration\"})\nMERGE (model_image_gen)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"ImageEditing\"})\n\nMERGE (model_image_gen_v2)-[:HAS_FEATURE]->(:Feature {name: \"Generate photorealistic images\"})\nMERGE (model_image_gen_v2)-[:SUPPORTS_LANGUAGE]->(:Language {name: \"English\"})\nMERGE (model_image_gen_v2)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"TextToImageGeneration\"})\nMERGE (model_image_gen_v2)-[:SUPPORTS_USE_CASE]->(:UseCase {name: \"ImageEditing\"})\n\nWITH *  \/\/ Add this line to pass all variables to the next part of the query\nMATCH (entity)\nWHERE entity.id IN [\"Amazon_Web_Services\", \"Amazon_Bedrock\", \"Amazon_Titan\", \"Amazon_Titan_Text_Premier\", \"Amazon_Titan_Text_Express\", \"Amazon_Titan_Text_Lite\", \"Amazon_Titan_Text_Embeddings\", \"Amazon_Titan_Image_Generator\", \"Amazon_Titan_Image_Generator_v2\"]\nMERGE (article:Article {id: \"a6aad9d6-597b-4090-9499-a3d22aba266b\"})-[:SOURCE_OF]->(entity)",
        "article_id": "a6aad9d6-597b-4090-9499-a3d22aba266b"
      },
      {
        "question": "```turtle\n@prefix ex: <http:\/\/example.org\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n@prefix rdf: <http:\/\/www.w3.org\/1999\/02\/22-rdf-syntax-ns#> .\n@prefix rdfs: <http:\/\/www.w3.org\/2000\/01\/rdf-schema#> .\n\nex:ContextualizationDistillation a dbo:ResearchPaper ;\n    dbo:title \"Contextualization Distillation from Large Language Models for Knowledge Graph Completion\" ;\n    dbo:abstract \"While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks.\" ;\n    dbo:author ex:DaweiLi, ex:ZhenTan, ex:TianlongChen, ex:HuanLiu ;\n    dbo:publisher dbr:Association_for_Computational_Linguistics ;\n    dbo:publicationDate \"2024-03\"^^xsd:date ;\n    dbo:location dbr:Malta ;\n    dbo:pages \"458--477\" .\n\nex:DaweiLi a dbo:Person ;\n    rdfs:label \"Dawei Li\" .\n\nex:ZhenTan a dbo:Person ;\n    rdfs:label \"Zhen Tan\" .\n\nex:TianlongChen a dbo:Person ;\n    rdfs:label \"Tianlong Chen\" .\n\nex:HuanLiu a dbo:Person ;\n    rdfs:label \"Huan Liu\" .\n\nex:KnowledgeGraphCompletion a dbo:Process ;\n    rdfs:label \"Knowledge Graph Completion\" .\n\nex:LargeLanguageModel a dbo:Technology ;\n    rdfs:label \"Large Language Model\" .\n\nex:ContextualizationDistillationStrategy a dbo:Method ;\n    rdfs:label \"Contextualization Distillation Strategy\" ;\n    dbo:related ex:KnowledgeGraphCompletion, ex:LargeLanguageModel .\n\nex:KGC_Baselines a dbo:Dataset ;\n    rdfs:label \"KGC Baselines\" .\n\nex:KG_BERT a dbo:Software ;\n    rdfs:label \"KG-BERT\" ;\n    dbo:related ex:KGC_Baselines .\n\nex:CSProm_KG a dbo:Software ;\n    rdfs:label \"CSProm-KG\" ;\n    dbo:related ex:KGC_Baselines .\n\nex:KG_S2S a dbo:Software ;\n    rdfs:label \"KG-S2S\" ;\n    dbo:related ex:KGC_Baselines .\n\nex:GenKGC a dbo:Software ;\n    rdfs:label \"GenKGC\" ;\n    dbo:related ex:KGC_Baselines .\n\nex:LambdaKG a dbo:Software ;\n    rdfs:label \"lambdaKG\" ;\n    dbo:related ex:KGC_Baselines .\n\nex:PaLM a dbo:Software ;\n    rdfs:label \"PaLM\" ;\n    dbo:related ex:LargeLanguageModel .\n\nex:ChatGPT a dbo:Software ;\n    rdfs:label \"ChatGPT\" ;\n    dbo:related ex:LargeLanguageModel .\n```\n---\nSkip to content\nNavigation Menu\nDavid-Li0406\n\/\nContextulization-Distillation\nType \/ to search\nCode\nIssues\n1\nPull requests\nActions\nProjects\nSecurity\nInsights\nContextulization-Distillation\nPublic\nWatch\nFork 0\n Star 16\nDavid-Li0406\/Contextulization-Distillation\n\u00a0main\n1 Branch\nTags\nt\nAdd file\nAdd file\nCode\nFolders and files\nName\tLast commit message\tLast commit date\n\nLatest commit\n\u00a0\nHistory\n13 Commits\n\n\nCSProm-KG\n\t\n\u00a0\n\t\n\u00a0\n\n\nKG-S2S\n\t\n\u00a0\n\t\n\u00a0\n\n\nKGE-LLM\n\t\n\u00a0\n\t\n\u00a0\n\n\nkg-bert\n\t\n\u00a0\n\t\n\u00a0\n\n\nlambdaKG\n\t\n\u00a0\n\t\n\u00a0\n\n\nREADME.md\n\t\n\u00a0\n\t\n\u00a0\n\n\noverview.png\n\t\n\u00a0\n\t\n\u00a0\nRepository files navigation\nREADME\n\nContextualization Distillation from Large Language Models for Knowlege Graph Completion\n\nUpdate\n\n4.24: add KG-S2S implementation and descriptive context files.\n\nOverview\n\nDependency\n\nTo fully run this project, one is recommended to install the requirement packages for each KGC baseline (best to install them separately), with the OpenAI and PaLM libraries as well.\n\nBenchmark LLMs' Performance in KGC\n\nTo benchmark the two large language models' (PaLM and ChatGPT) performance in KGC, first enter your api keys into KGE-LLM\/utils.py\n\npalm.configure(api_key='your own plm api key')\nAPI_KEY = 'your own openai api key'\n\n\nthen revise the dataset variable in KGE-LLM\/inference_palm.py or KGE-LLM\/inference_chatgpt.py, and run the corresponding script. Use the scripts with the suffix \"_yoga\" to get the result on Yoga3.\n\nExtract Descriptive Context from LLMs\n\nTo generate descriptive context from PaLM as the auxiliary corpus for the following steps, one can directly run:\n\ncd KGE-LLM\npython run.sh\n\n\nAfter runing the script, you should get two data files knowledge_context.txt in the corresponding dataset folder.\n\nWe will later provide the generated corpus here and the post-processing script.\n\nContextualization Distillation with Different KGC Baselines\n\nTo apply our contextualization distillation framework in each KGC baseline, first copy the two processed data files to the KGC baseline's dataset folder.\n\nKG-BERT\n\nFor KG-BERT on WN18RR, run\n\ncd kg-bert\nbash train_wn18rr.sh\n\n\nFor KG-BERT on FB15k-237N, run\n\ncd kg-bert\nbash train_fb15k237N.sh\n\nCSProm-KG\n\nFor CSProm-KG on WN18RR, run\n\ncd CSProm-KG\nbash train_wn18rr.sh\n\n\nFor CSProm-KG on FB15k-237N, run\n\ncd CSProm-KG\nbash train_fb15k237N.sh\n\nGenKGC\n\nFor GenKGC on WN18RR, run\n\ncd lambdaKG\/scripts\/kgc\nbash genkgc_WN18RR.sh\n\n\nFor GenKGC on FB15k-237N, run\n\ncd lambdaKG\/scripts\/kgc\nbash genkgc_FB15k-237N.sh\n\nKG-S2S\n\nFor KG-S2S on WN18RR, run\n\ncd KG-S2S\nbash train_wn18rr.sh\n\n\nFor KG-S2S on FB15k-237N, run\n\ncd KG-S2S\nbash train_fb15k237N.sh\n\n\nCredits: This work began as a fork of several KGC models' repository: KG-BERT, CSProm-KG, PromptKG and KG-S2S. If you found our code useful, please consider citing:\n\n@inproceedings{li-etal-2024-contextualization,\n    title = \"Contextualization Distillation from Large Language Model for Knowledge Graph Completion\",\n    author = \"Li, Dawei  and\n      Tan, Zhen  and\n      Chen, Tianlong  and\n      Liu, Huan\",\n    editor = \"Graham, Yvette  and\n      Purver, Matthew\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EACL 2024\",\n    month = mar,\n    year = \"2024\",\n    address = \"St. Julian{'}s, Malta\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https:\/\/aclanthology.org\/2024.findings-eacl.32\",\n    pages = \"458--477\",\n    abstract = \"While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the \\textit{Contextualization Distillation} strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks{---}reconstruction and contextualization{---}allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into how to generate high-quality corpora for KGC, as well as the selection of suitable distillation tasks.\",\n}\n\nAbout\n\nCode and data for EACL 2024 paper \"Contextualization Distillation from Large Language Models for Knowledge Graph Completion\"\n\nResources\n Readme\n Activity\nStars\n 16 stars\nWatchers\n 3 watching\nForks\n 0 forks\nReport repository\n\n\nReleases\nNo releases published\n\n\nPackages\nNo packages published\n\n\n\nLanguages\nPython\n98.2%\n \nShell\n1.8%\nFooter\n\u00a9 2024 GitHub,\u00a0Inc.\nFooter navigation\nTerms\nPrivacy\nSecurity\nStat",
        "response": "\/\/ Create grounded entities\nMERGE (dawei:Person {name: \"Dawei Li\"})\nMERGE (zhen:Person {name: \"Zhen Tan\"})\nMERGE (tianlong:Person {name: \"Tianlong Chen\"})\nMERGE (huan:Person {name: \"Huan Liu\"})\nMERGE (kgc:Process {name: \"Knowledge Graph Completion\"})\nMERGE (llm:Technology {name: \"Large Language Model\"})\nMERGE (cds:Method {name: \"Contextualization Distillation Strategy\"})\nMERGE (kgc_baselines:Dataset {name: \"KGC Baselines\"})\nMERGE (kg_bert:Software {name: \"KG-BERT\"})\nMERGE (cs_prom_kg:Software {name: \"CSProm-KG\"})\nMERGE (kg_s2s:Software {name: \"KG-S2S\"})\nMERGE (genkgc:Software {name: \"GenKGC\"})\nMERGE (lambda_kg:Software {name: \"lambdaKG\"})\nMERGE (palm:Software {name: \"PaLM\"})\nMERGE (chatgpt:Software {name: \"ChatGPT\"})\nMERGE (acl:Publisher {name: \"Association for Computational Linguistics\"})\nMERGE (malta:Location {name: \"Malta\"})\n\n\/\/ Create relationships\nMERGE (cds)-[:RELATED]->(kgc)\nMERGE (cds)-[:RELATED]->(llm)\nMERGE (kg_bert)-[:RELATED]->(kgc_baselines)\nMERGE (cs_prom_kg)-[:RELATED]->(kgc_baselines)\nMERGE (kg_s2s)-[:RELATED]->(kgc_baselines)\nMERGE (genkgc)-[:RELATED]->(kgc_baselines)\nMERGE (lambda_kg)-[:RELATED]->(kgc_baselines)\nMERGE (palm)-[:RELATED]->(llm)\nMERGE (chatgpt)-[:RELATED]->(llm)\n\n\/\/ Add a WITH clause to pass the matched entities to the next part of the query\nWITH dawei, zhen, tianlong, huan, kgc, llm, cds, kgc_baselines, kg_bert, cs_prom_kg, kg_s2s, genkgc, lambda_kg, palm, chatgpt, acl, malta\n\n\/\/ Create SOURCE_OF relationships\nMERGE (article:Article {id: \"5da3cfc3-3559-49a7-ab66-983f2b5d9c94\"})\nMERGE (article)-[:SOURCE_OF]->(dawei)\nMERGE (article)-[:SOURCE_OF]->(zhen)\nMERGE (article)-[:SOURCE_OF]->(tianlong)\nMERGE (article)-[:SOURCE_OF]->(huan)\nMERGE (article)-[:SOURCE_OF]->(kgc)\nMERGE (article)-[:SOURCE_OF]->(llm)\nMERGE (article)-[:SOURCE_OF]->(cds)\nMERGE (article)-[:SOURCE_OF]->(kgc_baselines)\nMERGE (article)-[:SOURCE_OF]->(kg_bert)\nMERGE (article)-[:SOURCE_OF]->(cs_prom_kg)\nMERGE (article)-[:SOURCE_OF]->(kg_s2s)\nMERGE (article)-[:SOURCE_OF]->(genkgc)\nMERGE (article)-[:SOURCE_OF]->(lambda_kg)\nMERGE (article)-[:SOURCE_OF]->(palm)\nMERGE (article)-[:SOURCE_OF]->(chatgpt)\nMERGE (article)-[:SOURCE_OF]->(acl)\nMERGE (article)-[:SOURCE_OF]->(malta)",
        "article_id": "5da3cfc3-3559-49a7-ab66-983f2b5d9c94"
      },
      {
        "question": "```turtle\n@prefix dbpedia: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dct: <http:\/\/purl.org\/dc\/terms\/> .\n\n# Entities\ndbr:Julia_Programming_Language a dbo:ProgrammingLanguage ;\n    dct:subject dbr:Machine_Learning .\n\ndbr:Flux_(machine_learning) a dbo:Software ;\n    dct:subject dbr:Machine_Learning ;\n    dbo:developer dbr:Julia_Programming_Language .\n\ndbr:Knet a dbo:Software ;\n    dct:subject dbr:Deep_Learning .\n\ndbr:Zygote a dbo:Software ;\n    dct:subject dbr:Automatic_Differentiation .\n\ndbr:PyTorch a dbo:Software ;\n    dct:subject dbr:Deep_Learning .\n\ndbr:JAX a dbo:Software ;\n    dct:subject dbr:Deep_Learning .\n\ndbr:TensorFlow a dbo:Software ;\n    dct:subject dbr:Deep_Learning .\n\n# Relations\ndbr:Julia_Programming_Language dbo:comparedTo dbr:Python .\ndbr:Julia_Programming_Language dbo:comparedTo dbr:R .\ndbr:Julia_Programming_Language dbo:comparedTo dbr:Matlab .\n\ndbr:Flux_(machine_learning) dbo:uses dbr:Julia_Programming_Language .\ndbr:Knet dbo:uses dbr:Julia_Programming_Language .\ndbr:Zygote dbo:uses dbr:Julia_Programming_Language .\n\ndbr:Deep_Learning_with_Julia a dbo:Book ;\n    dct:subject dbr:Machine_Learning ;\n    dbo:author dbr:Logan_Kilpatrick .\n\ndbr:Deep_Learning_with_Julia dbo:uses dbr:Flux_(machine_learning) .\n```\n---\nAccessibility links\nSkip to main content\nAccessibility help\nAccessibility feedback\nFilters and topics\nAll\nImages\nVideos\nNews\nWeb\nBooks\nFinance\nTools\nGithub\nTutorial\nCourse\nPackages\nReddit\nSearch Results\n\nDeep learning in Julia - Machine Learning\nThe Julia Programming Language\nhttps:\/\/discourse.julialang.org \u203a ... \u203a Machine Learning\n11 Apr 2024 \u2014 Lux is a new Julia deep learning framework that decouples models and parameterization using deeply nested named tuples. Functional Layer API \u2013\u00a0...\n\nWhy hasn't the deep learning community embraced Julia yet?\nHacker News\nhttps:\/\/news.ycombinator.com \u203a item\n12 Sept 2022 \u2014 Julia provides no meaningful advantages. PyTorch and JAX are too good. For typical deep learning workloads, Julia will not easily have a speed\u00a0...\n\nFlux \u2013 Elegant ML\nFlux ML\nhttps:\/\/fluxml.ai\nThe Elegant Machine Learning Stack. Flux is a 100% pure-Julia stack and provides lightweight abstractions on top of Julia's native GPU and AD support.\n\nDeep Learning with Julia \u2013 How to Build and Train a Model ...\nfreeCodeCamp\nhttps:\/\/www.freecodecamp.org \u203a news \u203a deep-learning-...\n7 Mar 2023 \u2014 In this tutorial, I will guide you through the process of building and training a neural network to recognize handwritten digits using Julia.\n\nState of machine learning in Julia\nThe Julia Programming Language\nhttps:\/\/discourse.julialang.org \u203a ... \u203a Machine Learning\n11 Jan 2022 \u2014 Julia's AD speeds are fine. Zygote can have some overhead, but it's actually rather fast in most contexts compared to Jax\/PyTorch\/TensorFlow.\nScholarly articles for julia deep learning\n\n\nKnet: beginning deep learning with 100 lines of julia - \u200eYuret - Cited by 53\nJulia language in machine learning: Algorithms, \u2026 - \u200eGao - Cited by 88\nJulia for Data Science - \u200eJoshi - Cited by 10\nPeople also ask\nIs Julia used in deep learning?\nIs Julia faster than Python?\nWhy is Julia not Python?\nCan Julia replace Python?\nFeedback\n\nMachine learning with Julia\nReddit\u00a0\u00b7\u00a0r\/Julia\n10+ comments \u00b7 1 year ago\nWhen it comes to learning ML, there isn't much of a difference between the \"reasonable\" languages commonly used in ML (Julia, Python, R, Matlab)\u00a0...\nHow Good Is Julia for AI, Machine Learning, And Simulations?\n16 Jun 2021\nJulia vs Python : r\/learnmachinelearning - Reddit\n13 Mar 2024\nShould you learn Julia or Python for Machine Learning? - Reddit\n16 Aug 2021\n[D] Is it wise to keep using Python in machine learning and not ...\n4 Feb 2021\nMore results from www.reddit.com\n\nMachine Learning \u00b7 Julia Packages\njuliapackages.com\nhttps:\/\/juliapackages.com \u203a machine-learning\nJulia package for automatically generating Bayesian inference algorithms through message passing on Forney-style factor graphs. LossFunctions.jl.\n\nDive into Deep Learning - Julia\nGitHub\nhttps:\/\/neroblackstone.github.io \u203a D2lJulia\nThis tutorial mainly focuses on using pure julia to implement the code in Dive into Deep Learning, rather than explaining the specific principles of deep\u00a0...\n\nThe Deep Learning with Julia book, using Flux.jl.\nGitHub\nhttps:\/\/github.com \u203a logankilpatrick \u203a DeepLearningWit...\nDL with Julia is a book about how to do various deep learning tasks using the Julia programming language and specifically the Flux.jl package.\n\nAn Introduction to Machine Learning with Julia\nDataCamp\nhttps:\/\/www.datacamp.com \u203a Blog \u203a Julia\n3 Nov 2022 \u2014 Explore why Julia is suitable for machine learning, and get an introduction to the features and packages that can help you get started.\nPeople also search for\nJulia deep learning github\nJulia machine learning packages\nJulia deep learning tutorial\nJulia Flux\nJulia deep learning reddit\nJulia neural network\nJulia Flux tutorial\nSciML Julia\nPage navigation\n\t\n1\t\n2\n\t\n3\n\t\n4\n\t\n5\n\t\n6\n\t\n7\n\t\n8\n\t\n9\n\t\n10\n\t\nNext\nFooter links\nIreland\nPhibsborough, Dublin\u00a0-\u00a0From your device\n\u00a0-\u00a0Update location",
        "response": "\/\/ Create grounded entities\nMERGE (julia:ProgrammingLanguage {name: \"Julia Programming Language\"})\nMERGE (flux:Software {name: \"Flux\"})\nMERGE (knet:Software {name: \"Knet\"})\nMERGE (zygote:Software {name: \"Zygote\"})\nMERGE (pytorch:Software {name: \"PyTorch\"})\nMERGE (jax:Software {name: \"JAX\"})\nMERGE (tensorflow:Software {name: \"TensorFlow\"})\nMERGE (deepLearningBook:Book {name: \"Deep Learning with Julia\"})\nMERGE (logan:Author {name: \"Logan Kilpatrick\"})\n\n\/\/ Create grounded relations\nMERGE (julia)-[:SUBJECT]->(machineLearning:Topic {name: \"Machine Learning\"})\nMERGE (flux)-[:SUBJECT]->(machineLearning)\nMERGE (knet)-[:SUBJECT]->(deepLearning)\nMERGE (zygote)-[:SUBJECT]->(automaticDifferentiation:Topic {name: \"Automatic Differentiation\"})\nMERGE (pytorch)-[:SUBJECT]->(deepLearning)\nMERGE (jax)-[:SUBJECT]->(deepLearning)\nMERGE (tensorflow)-[:SUBJECT]->(deepLearning)\n\nMERGE (julia)-[:COMPARED_TO]->(python:ProgrammingLanguage {name: \"Python\"})\nMERGE (julia)-[:COMPARED_TO]->(r:ProgrammingLanguage {name: \"R\"})\nMERGE (julia)-[:COMPARED_TO]->(matlab:ProgrammingLanguage {name: \"Matlab\"})\n\nMERGE (flux)-[:USES]->(julia)\nMERGE (knet)-[:USES]->(julia)\nMERGE (zygote)-[:USES]->(julia)\n\nMERGE (deepLearningBook)-[:SUBJECT]->(machineLearning)\nMERGE (deepLearningBook)-[:AUTHOR]->(logan)\nMERGE (deepLearningBook)-[:USES]->(flux)\n\n\/\/ Add SOURCE_OF relations\nWITH *  \/\/ This passes all variables to the next part of the query\nMATCH (n)\nWHERE n.name IN [\"Julia Programming Language\", \"Flux\", \"Knet\", \"Zygote\", \"PyTorch\", \"JAX\", \"TensorFlow\", \"Deep Learning with Julia\", \"Logan Kilpatrick\"]\nMERGE (source:Article {id: \"13a3b02b-e283-41bb-90e5-eb7e92795171\"})-[:SOURCE_OF]->(n)",
        "article_id": "13a3b02b-e283-41bb-90e5-eb7e92795171"
      },
      {
        "question": "```turtle\n@prefix dbpedia: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dct: <http:\/\/purl.org\/dc\/terms\/> .\n\n# Entities\ndbr:Netflix a dbo:Company ;\n    dct:subject dbr:Streaming_media ;\n    dbo:abstract \"Netflix is an American subscription-based streaming service and production company.\" .\n\ndbr:Mac_Computer a dbo:Computer ;\n    dct:subject dbr:Personal_computer ;\n    dbo:abstract \"A Mac computer is a line of personal computers designed, manufactured, and sold by Apple Inc.\" .\n\n# Relations\ndbr:Netflix dct:hasPart dbr:Netflix_Help_Center ;\n    dct:hasPart dbr:Netflix_App_Store_Page .\n\ndbr:Netflix_Help_Center a dbo:WebPage ;\n    dct:description \"A help center for Netflix users.\" .\n\ndbr:Netflix_App_Store_Page a dbo:WebPage ;\n    dct:description \"The page for the Netflix app on the Apple App Store.\" .\n\n# User Actions\ndbr:User a dbo:Person ;\n    dct:hasPart dbr:Netflix_Account ;\n    dct:hasPart dbr:Mac_Computer .\n\ndbr:Netflix_Account a dbo:Account ;\n    dct:description \"An account for accessing Netflix services.\" .\n\n# Browsers\ndbr:Safari a dbo:WebBrowser ;\n    dct:subject dbr:Web_browser .\n\ndbr:Chrome a dbo:WebBrowser ;\n    dct:subject dbr:Web_browser .\n\ndbr:Firefox a dbo:WebBrowser ;\n    dct:subject dbr:Web_browser .\n\n# System Requirements\ndbr:macOS a dbo:OperatingSystem ;\n    dct:subject dbr:Operating_system .\n\ndbr:macOS_10.15 a dbo:Version ;\n    dct:description \"A version of macOS required to watch Netflix.\" .\n\n# Downloading\ndbr:Boot_Camp a dbo:Software ;\n    dct:description \"A utility that allows users to install Windows on Intel-based Macs.\" .\n\ndbr:Windows a dbo:OperatingSystem ;\n    dct:subject dbr:Operating_system .\n```\n---\nAccessibility links\nSkip to main content\nAccessibility help\nAccessibility feedback\nFilters and topics\nAll\nNews\nImages\nVideos\nWeb\nBooks\nFinance\nTools\nSearch Results\n\nHow to use Netflix on your Mac computer\nNetflix\nhttps:\/\/help.netflix.com \u203a node\nTo sign in to your Netflix account on your Mac computer: Go to netflix.com. Select Sign In and follow the steps on the screen. To sign out of your Netflix\u00a0...\n\nNetflix on the App Store\nApple\nhttps:\/\/apps.apple.com \u203a app \u203a netflix\nWe've got award-winning series, movies, documentaries, and stand-up specials. And with the mobile app, you get Netflix while you travel, commute, or just take\u00a0...\n3.6\n(402,626) \u00b7 Free \u00b7 \u200eiOS \u00b7 \u200eEntertainment\u00a0\u00a0\u00a0\nPeople also ask\nIs there a Netflix app for Mac?\nCan I download shows on Netflix on my Mac?\nWhy can't I watch Netflix on my Mac?\nWhat devices will no longer support Netflix?\nFeedback\n\nWill we ever get a Netflix app for Mac? : r\/macapps\nReddit\u00a0\u00b7\u00a0r\/macapps\n50+ comments \u00b7 1 year ago\nIt's essentially the same thing on Mac. Simply open Safari, go to a website, Netflix in this case, and go to 'File > Add to Dock' and voila, it\u00a0...\n10 answers\n\u00a0\n\u00b7\n\u00a0\nTop answer:\u00a0\nYou need to voice your feedback to Netflix about this. When doing so don't even mention iPads ...\nNetflix app for macOS : r\/macapps - Reddit\n12 Apr 2021\nNo Netflix App : r\/mac - Reddit\n17 Jan 2023\nCan someone tell me why there is still no Netflix app for MacOS?\n25 Jun 2021\nNetflix blocking Mac OS 10.11? : r\/MacOS - Reddit\n26 Jan 2024\nMore results from www.reddit.com\nVideos\n2:12\nHow To Download The Netflix App On Mac *NEW METHOD*\nYouTube \u00b7 Bog\n24 Nov 2023\n4:30\nHow to Install Netflix on MacBook? Netflix on macOS\nYouTube \u00b7 Karma Tech\n14 May 2024\n1:36\nHow To Download Netflix App On Mac - Easy Guide\nYouTube \u00b7 MacSafe - Apple Tutorials\n16 Aug 2024\n4\u00a0key moments\u00a0in this video\nView all\n\nCan't find the Netflix app\nNetflix Help Center\nhttps:\/\/help.netflix.com \u203a node\nIf you can't find the Netflix app on your device, or the app is missing from the apps list or app store, follow the steps for your device below.\n\nHow to download the Netflix app\nNetflix Help Center\nhttps:\/\/help.netflix.com \u203a node\nA step-by-step guide to downloading the Netflix app on your tablet, computer, or other device.\n\nHow to download Netflix on Mac, iPhone, and iPad?\nMacPaw\nhttps:\/\/macpaw.com \u203a how-to \u203a can-you-download-netf...\n2 Jan 2024 \u2014 You can't download Netflix movies and TV shows on macOS, but you can do it on an Intel-based Mac if you create a Boot Camp partition and install Windows on it.\n\nNetflix supported browsers and system requirements\nNetflix Help Center\nhttps:\/\/help.netflix.com \u203a node\nTo watch Netflix in a web browser using your Mac computer, it must have macOS 10.15 or later installed. Your browser must be on the required version listed\u00a0...\n\nHow to download Netflix on Mac\nSharesub\nhttps:\/\/www.sharesub.com \u203a cms \u203a how-to-download-n...\n14 Jun 2024 \u2014 No, there is no official Netflix application for macOS. Mac users need to access Netflix via a web browser such as Safari, Chrome or Firefox.\nImages\nHow to download Netflix movies on Mac\nYouTube\nHow to Install Netflix On ANY MAC OS | Step By Step Tutorial\nYouTube\nHow to download Netflix on Mac - Sharesub\nSharesub\nShow more images\nPeople also search for\nNetflix for Mac free download\nNetflix download\nNetflix Mac download offline\nWhy is there no Netflix app for Mac\nNetflix for MacBook Air\nHow to download Netflix on MacBook Air\nNetflix login\nNetflix download for TV\nPage navigation\n\t\n1\t\n2\n\t\n3\n\t\n4\n\t\n5\n\t\n6\n\t\n7\n\t\n8\n\t\n9\n\t\n10\n\t\nNext\nFooter links\nIreland\nPhibsborough, Dublin\u00a0-\u00a0Based on your places (Home)\n\u00a0-\u00a0Update location",
        "response": "\/\/ Create grounded entities and relations based on the provided text\nMERGE (n:Company {name: \"Netflix\"})\n    ON CREATE SET n.id = \"dbr:Netflix\", n.abstract = \"Netflix is an American subscription-based streaming service and production company.\"\nMERGE (m:Computer {name: \"Mac Computer\"})\n    ON CREATE SET m.id = \"dbr:Mac_Computer\", m.abstract = \"A Mac computer is a line of personal computers designed, manufactured, and sold by Apple Inc.\"\nMERGE (n)-[:HAS_PART]->(nhc:WebPage {name: \"Netflix Help Center\", description: \"A help center for Netflix users.\"})\nMERGE (n)-[:HAS_PART]->(nas:WebPage {name: \"Netflix App Store Page\", description: \"The page for the Netflix app on the Apple App Store.\"})\nMERGE (u:Person {name: \"User\"})\n    ON CREATE SET u.id = \"dbr:User\"\nMERGE (u)-[:HAS_PART]->(na:Account {name: \"Netflix Account\", description: \"An account for accessing Netflix services.\"})\nMERGE (u)-[:HAS_PART]->(mc:Computer {name: \"Mac Computer\"})\nMERGE (s:WebBrowser {name: \"Safari\"})\n    ON CREATE SET s.id = \"dbr:Safari\"\nMERGE (c:WebBrowser {name: \"Chrome\"})\n    ON CREATE SET c.id = \"dbr:Chrome\"\nMERGE (f:WebBrowser {name: \"Firefox\"})\n    ON CREATE SET f.id = \"dbr:Firefox\"\nMERGE (os:OperatingSystem {name: \"macOS\"})\n    ON CREATE SET os.id = \"dbr:macOS\"\nMERGE (v:Version {name: \"macOS 10.15\", description: \"A version of macOS required to watch Netflix.\"})\n    ON CREATE SET v.id = \"dbr:macOS_10.15\"\nMERGE (bc:Software {name: \"Boot Camp\", description: \"A utility that allows users to install Windows on Intel-based Macs.\"})\n    ON CREATE SET bc.id = \"dbr:Boot_Camp\"\nMERGE (w:OperatingSystem {name: \"Windows\"})\n    ON CREATE SET w.id = \"dbr:Windows\"\n\n\/\/ Use WITH to pass the matched nodes to the next part of the query\nWITH n, m, nhc, nas, u, na, s, c, f, os, v, bc, w\n\n\/\/ Create SOURCE_OF relations\nMERGE (a:Article {id: \"31469e1d-d546-46c1-b651-154d454987fc\"})\nMERGE (n)-[:SOURCE_OF]->(a)\nMERGE (m)-[:SOURCE_OF]->(a)\nMERGE (nhc)-[:SOURCE_OF]->(a)\nMERGE (nas)-[:SOURCE_OF]->(a)\nMERGE (u)-[:SOURCE_OF]->(a)\nMERGE (na)-[:SOURCE_OF]->(a)\nMERGE (s)-[:SOURCE_OF]->(a)\nMERGE (c)-[:SOURCE_OF]->(a)\nMERGE (f)-[:SOURCE_OF]->(a)\nMERGE (os)-[:SOURCE_OF]->(a)\nMERGE (v)-[:SOURCE_OF]->(a)\nMERGE (bc)-[:SOURCE_OF]->(a)\nMERGE (w)-[:SOURCE_OF]->(a)",
        "article_id": "31469e1d-d546-46c1-b651-154d454987fc"
      },
      {
        "question": "```turtle\n@prefix dbpedia: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dbo: <http:\/\/dbpedia.org\/ontology\/> .\n@prefix dbr: <http:\/\/dbpedia.org\/resource\/> .\n@prefix dct: <http:\/\/purl.org\/dc\/terms\/> .\n\n# Entities\ndbr:Finglas_Luas_Line a dbo:Transportation .\ndbr:Eamon_Ryan a dbo:Person ;\n    dbo:position dbr:Minister_for_Transport .\n\ndbr:Broombridge a dbo:Place ;\n    dbo:locatedIn dbr:Dublin .\n\ndbr:Jamestown_Business_Park a dbo:Place ;\n    dbo:area \"43 hectares\" ;\n    dbo:potentialHousing \"3800\" .\n\ndbr:Finglas a dbo:Place ;\n    dbo:population \"60000\" .\n\ndbr:National_Transport_Authority a dbo:Organization .\n\n# Relations\ndbr:Finglas_Luas_Line dbo:connectsTo dbr:Broombridge ;\n    dbo:serves dbr:Finglas ;\n    dbo:scheduledForCompletion \"2031\"^^xsd:gYear ;\n    dbo:hasStop dbr:St_Helena ;\n    dbo:hasStop dbr:Finglas_Village ;\n    dbo:hasStop dbr:St_Margaret's_Road ;\n    dbo:hasStop dbr:Charlestown .\n\ndbr:Eamon_Ryan dbo:seeksApprovalFor dbr:Finglas_Luas_Line .\n\ndbr:Jamestown_Business_Park dbo:hasPotentialPopulation \"8000\" .\n\ndbr:Finglas_Luas_Line dbo:reducesCarJourneys \"440000\"^^xsd:integer .\n\ndbr:Broombridge dbo:interchangesWith dbr:National_Transport_Authority .\n\n# Additional Information\ndbr:Finglas_Luas_Line dbo:hasJourneyTime \"30 minutes\" .\n```\n---\nSections\nSubscribe\nSign In\nHome\nLatest\nSubscriber Only\nIreland\nDublin\nEducation\nHousing & Planning\nSocial Affairs\nStardust\nPolitics\nOpinion\nBusiness\nWorld\nSport\nYour Money\nCrime & Law\nProperty\nFood\nHealth\nLife & Style\nCulture\nEnvironment\nTechnology\nScience\nMedia\nAbroad\nObituaries\nTransport\nMotors\nListen\nPodcasts\nVideo\nPhotography\nHistory\nTuarasc\u00e1il\nStudent Hub\nOffbeat\nCrosswords & Puzzles\nFamily Notices\nOpens in new window\nSponsored\nSubscriber Rewards\nCompetitions\nWeather Forecast\nSubscriber Only\nDublin\nFinglas Luas: New tram line could service population of 60,000 by 2031\nGovernment approval for the plans will be sought by Minister for Transport Eamon Ryan on Tuesday\nExpand\nThe new Finglas Luas line would connect to the existing Green line at Broombridge in Cabra. Photograph: Alan Betson\nOlivia Kelly\nMon Oct 21 2024 - 06:00\n\nGovernment approval for Dublin\u2019s next Luas line, which could bring trams to Finglas by 2031, will be sought by Minister for Transport Eamon Ryan on Tuesday.\n\nA population of about 60,000 people would be served by the new Finglas Luas line, which would connect to the existing Green line at Broombridge in Cabra, with services running every 7.5 minutes to the city centre and on to Brides Glen in south Dublin.\n\nUnder the Transport Strategy for the Greater Dublin Area the Finglas Luas line had been scheduled for completion by 2036. However, Mr Ryan indicated earlier this year he supported accelerated development of the project and would seek prioritisation of its construction.\n\nA positive Cabinet decision this week would allow Transport Infrastructure Ireland (TII), the State body with responsibility for developing the line, to seek planning consent from An Bord Plean\u00e1la for its development, which could push its expected completion forward by several years.\n\nREAD MORE\n\u2018The fathers of those babies are the sons of Tuam\u2019s finest families. They\u2019ll never talk\u2019\nKen Early: Liverpool and Chelsea showcase \u2018S**t on a Stick: 2024 edition\u2019\nFour ideas to save Ireland\u2019s pubs, restaurants and cafes\nGerry Hutch considers general election run to unseat Mary Lou McDonald in Dublin\n\nTII chief executive Peter Walsh in June told The Irish Times he was \u201cimmediately\u201d ready to seek a Railway Order from the board to build the line as soon as Government approval was delivered. \u201cIf we got approval, the preliminary design is in existence so it would be a matter of weeks to have it in with An Bord Plean\u00e1la,\u201d he said.\n\nIt is understood that an application to the board before the end of this year would put the line on track for completion by 2031.\n\nThe Green line extension from St Stephen\u2019s Green to Broombridge began services in 2017 and was designed to provide for a link to Finglas. However, while a preferred route for the 4km line was selected, its development was long-fingered, with other rail projects \u2013 particularly the MetroLink which also serves the northside of Dublin \u2013 prioritised.\n\nThe project received a boost last year with the approval by Dublin City Council of a regeneration masterplan for the Jamestown Business Park, 43 hectares of industrial lands to the north of Finglas village.\n\nThe lands have been identified as having the potential for 3,800 homes, accommodating a population of up to 8,000 people. More than half the sites have been classified by the council as \u201cimmediately available\u201d for development, and substantial numbers of homes could be in place before the Luas starts operating.\n\n[\u00a0Luas to get new trams worth \u20ac500m over next 15 years\nOpens in new window\n\u00a0]\n\nThe preferred route of the Finglas line would have four stops, running north from Broombridge to St Helena\u2019s in south Finglas, an area dominated by two-storey social housing developments largely built in the 1970s; then on to Finglas Village; to St Margaret\u2019s Road, which will serve the new Jamestown housing as well as existing estates; before terminating at Charlestown, close to the M50 and dominated by a shopping centre and high-density apartment schemes built over the past 20 years.\n\nThe journey time from Charlestown, the furthest station, to College Green would be approximately 30 minutes, shaving about 17 minutes off the morning commute by car, according to National Transport Authority assessments, which estimate the new line will take about 440,000 private car journeys off the roads along the route annually.\n\nThe line will also provide interchanges with rail services at Broombridge, which is also on the planned DART+ West line between Dublin city and Maynooth. In addition, there will be a 350-vehicle park and ride facility near the St Margaret\u2019s Road stop, close to the M50 and M2 interchange.\n\nSign up for push alerts and have the best news, analysis and comment delivered directly to your phone\nJoin The Irish Times on WhatsApp and stay up to date\nListen to our Inside Politics podcast for the best political chat and analysis\nOlivia Kelly\n\nOlivia Kelly is Dublin Editor of The Irish Times\n\nOpens in new window\nOpens in new window\nLuas\nEamon Ryan\n",
        "response": "MERGE (finglasLuasLine:Transportation {name: \"Finglas Luas Line\"})\nMERGE (eamonRyan:Person {name: \"Eamon Ryan\"})\nMERGE (broombridge:Place {name: \"Broombridge\"})\nMERGE (jamestownBusinessPark:Place {name: \"Jamestown Business Park\"})\nMERGE (finglas:Place {name: \"Finglas\"})\nMERGE (nationalTransportAuthority:Organization {name: \"National Transport Authority\"})\n\nMERGE (finglasLuasLine)-[:CONNECTS_TO]->(broombridge)\nMERGE (finglasLuasLine)-[:SERVES]->(finglas)\nMERGE (completionYear:Year {value: 2031})\nMERGE (finglasLuasLine)-[:SCHEDULED_FOR_COMPLETION]->(completionYear)\nMERGE (finglasLuasLine)-[:HAS_STOP]->(:Stop {name: \"St Helena\"})\nMERGE (finglasLuasLine)-[:HAS_STOP]->(:Stop {name: \"Finglas Village\"})\nMERGE (finglasLuasLine)-[:HAS_STOP]->(:Stop {name: \"St Margaret's Road\"})\nMERGE (finglasLuasLine)-[:HAS_STOP]->(:Stop {name: \"Charlestown\"})\n\nMERGE (eamonRyan)-[:SEEKS_APPROVAL_FOR]->(finglasLuasLine)\n\nMERGE (population:Population {value: 8000})\nMERGE (jamestownBusinessPark)-[:HAS_POTENTIAL_POPULATION]->(population)\n\nMERGE (carJourneys:CarJourneys {value: 440000})\nMERGE (finglasLuasLine)-[:REDUCES_CAR_JOURNEYS]->(carJourneys)\n\nMERGE (broombridge)-[:INTERCHANGES_WITH]->(nationalTransportAuthority)\n\nMERGE (article:Article {id: \"550265b7-dae5-4233-8f6d-dfd014fb0356\"})\nMERGE (finglasLuasLine)-[:SOURCE_OF]->(article)\nMERGE (eamonRyan)-[:SOURCE_OF]->(article)\nMERGE (broombridge)-[:SOURCE_OF]->(article)\nMERGE (jamestownBusinessPark)-[:SOURCE_OF]->(article)\nMERGE (finglas)-[:SOURCE_OF]->(article)\nMERGE (nationalTransportAuthority)-[:SOURCE_OF]->(article)",
        "article_id": "550265b7-dae5-4233-8f6d-dfd014fb0356"
      },
      {
        "question": "```turtle\n@prefix dgl: <http:\/\/dbpedia.org\/resource\/DGL> .\n@prefix dgl_data: <http:\/\/dbpedia.org\/resource\/DGL_Data> .\n@prefix dgl_nn: <http:\/\/dbpedia.org\/resource\/DGL_NN> .\n@prefix dgl_dataloader: <http:\/\/dbpedia.org\/resource\/DGL_DataLoader> .\n@prefix dgl_graph: <http:\/\/dbpedia.org\/resource\/DGLGraph> .\n@prefix dgl_dataset: <http:\/\/dbpedia.org\/resource\/DGL_Dataset> .\n@prefix dgl_training: <http:\/\/dbpedia.org\/resource\/DGL_Training> .\n@prefix dgl_api: <http:\/\/dbpedia.org\/resource\/DGL_API> .\n@prefix dgl_transform: <http:\/\/dbpedia.org\/resource\/DGL_Transform> .\n@prefix dgl_sampling: <http:\/\/dbpedia.org\/resource\/DGL_Sampling> .\n@prefix dgl_function: <http:\/\/dbpedia.org\/resource\/DGL_Function> .\n@prefix dgl_geometry: <http:\/\/dbpedia.org\/resource\/DGL_Geometry> .\n@prefix dgl_optim: <http:\/\/dbpedia.org\/resource\/DGL_Optim> .\n@prefix dgl_multiprocessing: <http:\/\/dbpedia.org\/resource\/DGL_Multiprocessing> .\n@prefix dgl_sparsity: <http:\/\/dbpedia.org\/resource\/DGL_Sparsity> .\n@prefix dgl_distributed: <http:\/\/dbpedia.org\/resource\/DGL_Distributed> .\n@prefix dgl_user_defined: <http:\/\/dbpedia.org\/resource\/DGL_User_Defined_Functions> .\n@prefix dgl_graphbolt: <http:\/\/dbpedia.org\/resource\/DGL_GraphBolt> .\n@prefix dgl_tutorial: <http:\/\/dbpedia.org\/resource\/DGL_Tutorial> .\n@prefix dgl_faq: <http:\/\/dbpedia.org\/resource\/DGL_FAQ> .\n@prefix dgl_environment: <http:\/\/dbpedia.org\/resource\/DGL_Environment> .\n\ndgl:hasTutorial dgl_tutorial:Graph_Classification .\ndgl:hasAPI dgl_api:DGL_API .\ndgl:hasDataLoader dgl_dataloader:GraphDataLoader .\ndgl:hasDataset dgl_dataset:GINDataset .\ndgl:hasTrainingMethod dgl_training:Training_GNN .\ndgl:hasGraphRepresentation dgl_graph:DGLGraph .\ndgl:hasNodeFeature dgl:Node_Feature .\ndgl:hasGraphLevelLabel dgl:Graph_Level_Label .\n\ndgl_dataset:GINDataset dgl:hasNodeFeatureDimensionality \"3\"^^xsd:integer .\ndgl_dataset:GINDataset dgl:hasGraphCategories \"2\"^^xsd:integer .\n\ndgl_training:Training_GNN dgl:usesDataLoader dgl_dataloader:GraphDataLoader .\ndgl_training:Training_GNN dgl:usesDataset dgl_dataset:GINDataset .\ndgl_training:Training_GNN dgl:hasTrainingLoop dgl:Training_Loop .\n\ndgl:Node_Feature dgl:hasFeatureDimension \"3\"^^xsd:integer .\ndgl:Graph_Level_Label dgl:hasLabelCount \"2\"^^xsd:integer .\n\ndgl:hasNextStep dgl:What\u2019s_next .\ndgl:hasAdvancedMaterial dgl:advanced_materials:Stochastic_Training .\n```\n---\n DGL\n2.2.x\n\nGET STARTED\n\nInstall and Setup\nA Blitz Introduction to DGL\nNode Classification with DGL\nHow Does DGL Represent A Graph?\nWrite your own GNN module\nLink Prediction using Graph Neural Networks\nTraining a GNN for Graph Classification\nOverview of Graph Classification with GNN\nLoading Data\nDefining Data Loader\nA Batched Graph in DGL\nDefine Model\nTraining Loop\nWhat\u2019s next\nMake Your Own Dataset\n\nADVANCED MATERIALS\n\n\ud83c\udd95 Stochastic Training of GNNs with GraphBolt\nUser Guide\n\u7528\u6237\u6307\u5357\u3010\u5305\u542b\u8fc7\u65f6\u4fe1\u606f\u3011\n\uc0ac\uc6a9\uc790 \uac00\uc774\ub4dc[\uc2dc\ub300\uc5d0 \ub4a4\uccd0\uc9c4]\n\ud83c\udd95 Tutorial: Graph Transformer\nTutorials: dgl.sparse\nTraining on CPUs\nTraining on Multiple GPUs\nDistributed training\nPaper Study with DGL\n\nAPI REFERENCE\n\ndgl\ndgl.data\ndgl.dataloading\ndgl.DGLGraph\ndgl.distributed\ndgl.function\ndgl.geometry\n\ud83c\udd95 dgl.graphbolt\ndgl.nn (PyTorch)\ndgl.nn.functional\ndgl.ops\ndgl.optim\ndgl.sampling\ndgl.sparse\ndgl.multiprocessing\ndgl.transforms\nUser-defined Functions\n\nNOTES\n\nContribute to DGL\nDGL Foreign Function Interface (FFI)\nPerformance Benchmarks\n\nMISC\n\nFrequently Asked Questions (FAQ)\nEnvironment Variables\nResources\n A Blitz Introduction to DGL Training a GNN for Graph Classification\n Edit on GitHub\n\nNote\n\nGo to the end to download the full example code\n\nTraining a GNN for Graph Classification\uf0c1\n\nBy the end of this tutorial, you will be able to\n\nLoad a DGL-provided graph classification dataset.\n\nUnderstand what readout function does.\n\nUnderstand how to create and use a minibatch of graphs.\n\nBuild a GNN-based graph classification model.\n\nTrain and evaluate the model on a DGL-provided dataset.\n\n(Time estimate: 18 minutes)\n\nimport os\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport dgl\nimport dgl.data\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nOverview of Graph Classification with GNN\uf0c1\n\nGraph classification or regression requires a model to predict certain graph-level properties of a single graph given its node and edge features. Molecular property prediction is one particular application.\n\nThis tutorial shows how to train a graph classification model for a small dataset from the paper How Powerful Are Graph Neural Networks.\n\nLoading Data\uf0c1\n# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes.\ndataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)\n\n\nThe dataset is a set of graphs, each with node features and a single label. One can see the node feature dimensionality and the number of possible graph categories of GINDataset objects in dim_nfeats and gclasses attributes.\n\nprint(\"Node feature dimensionality:\", dataset.dim_nfeats)\nprint(\"Number of graph categories:\", dataset.gclasses)\n\n\nfrom dgl.dataloading import GraphDataLoader\n\nNode feature dimensionality: 3\nNumber of graph categories: 2\n\nDefining Data Loader\uf0c1\n\nA graph classification dataset usually contains two types of elements: a set of graphs, and their graph-level labels. Similar to an image classification task, when the dataset is large enough, we need to train with mini-batches. When you train a model for image classification or language modeling, you will use a DataLoader to iterate over the dataset. In DGL, you can use the GraphDataLoader.\n\nYou can also use various dataset samplers provided in torch.utils.data.sampler. For example, this tutorial creates a training GraphDataLoader and test GraphDataLoader, using SubsetRandomSampler to tell PyTorch to sample from only a subset of the dataset.\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nnum_examples = len(dataset)\nnum_train = int(num_examples * 0.8)\n\ntrain_sampler = SubsetRandomSampler(torch.arange(num_train))\ntest_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n\ntrain_dataloader = GraphDataLoader(\n    dataset, sampler=train_sampler, batch_size=5, drop_last=False\n)\ntest_dataloader = GraphDataLoader(\n    dataset, sampler=test_sampler, batch_size=5, drop_last=False\n)\n\n\nYou can try to iterate over the created GraphDataLoader and see what it gives:\n\nit = iter(train_dataloader)\nbatch = next(it)\nprint(batch)\n\n[Graph(num_nodes=232, num_edges=1072,\n      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n      edata_schemes={}), tensor([0, 0, 0, 0, 0])]\n\n\nAs each element in dataset has a graph and a label, the GraphDataLoader will return two objects for each iteration. The first element is the batched graph, and the second element is simply a label vector representing the category of each graph in the mini-batch. Next, we\u2019ll talked about the batched graph.\n\nA Batched Graph in DGL\uf0c1\n\nIn each mini-batch, the sampled graphs are combined into a single bigger batched graph via dgl.batch. The single bigger batched graph merges all original graphs as separately connected components, with the node and edge features concatenated. This bigger graph is also a DGLGraph instance (so you can still treat it as a normal DGLGraph object as in here). It however contains the information necessary for recovering the original graphs, such as the number of nodes and ed",
        "response": "MERGE (dgl:DGL {id: \"DGL\"})\nMERGE (dgl_tutorial:Graph_Classification {id: \"Graph_Classification\"})\nMERGE (dgl_api:DGL_API {id: \"DGL_API\"})\nMERGE (dgl_dataloader:GraphDataLoader {id: \"GraphDataLoader\"})\nMERGE (dgl_dataset:GINDataset {id: \"GINDataset\"})\nMERGE (dgl_training:Training_GNN {id: \"Training_GNN\"})\nMERGE (dgl_graph:DGLGraph {id: \"DGLGraph\"})\nMERGE (dgl_node_feature:Node_Feature {id: \"Node_Feature\"})\nMERGE (dgl_graph_level_label:Graph_Level_Label {id: \"Graph_Level_Label\"})\nMERGE (dgl_training_loop:Training_Loop {id: \"Training_Loop\"})\nMERGE (dgl_what_next:Whats_next {id: \"Whats_next\"})  \/\/ Changed here\nMERGE (dgl_advanced_materials:Stochastic_Training {id: \"Stochastic_Training\"})\n\nMERGE (dgl)-[:hasTutorial]->(dgl_tutorial)\nMERGE (dgl)-[:hasAPI]->(dgl_api)\nMERGE (dgl)-[:hasDataLoader]->(dgl_dataloader)\nMERGE (dgl)-[:hasDataset]->(dgl_dataset)\nMERGE (dgl)-[:hasTrainingMethod]->(dgl_training)\nMERGE (dgl)-[:hasGraphRepresentation]->(dgl_graph)\nMERGE (dgl)-[:hasNodeFeature]->(dgl_node_feature)\nMERGE (dgl)-[:hasGraphLevelLabel]->(dgl_graph_level_label)\n\nMERGE (dgl_dataset)-[:hasNodeFeatureDimensionality]->(dgl_node_feature_dimensionality:Node_Feature_Dimensionality {value: 3})\nMERGE (dgl_dataset)-[:hasGraphCategories]->(dgl_graph_categories:Graph_Categories {value: 2})\n\nMERGE (dgl_training)-[:usesDataLoader]->(dgl_dataloader)\nMERGE (dgl_training)-[:usesDataset]->(dgl_dataset)\nMERGE (dgl_training)-[:hasTrainingLoop]->(dgl_training_loop)\n\nMERGE (dgl_node_feature)-[:hasFeatureDimension]->(dgl_feature_dimension:Feature_Dimension {value: 3})\nMERGE (dgl_graph_level_label)-[:hasLabelCount]->(dgl_label_count:Label_Count {value: 2})\n\nMERGE (dgl)-[:hasNextStep]->(dgl_what_next)\nMERGE (dgl)-[:hasAdvancedMaterial]->(dgl_advanced_materials)\n\nMERGE (source:Article {id: \"4079dd24-8583-4a61-b8ec-bf407fae5e3c\"})\nMERGE (source)-[:SOURCE_OF]->(dgl)\nMERGE (source)-[:SOURCE_OF]->(dgl_tutorial)\nMERGE (source)-[:SOURCE_OF]->(dgl_api)\nMERGE (source)-[:SOURCE_OF]->(dgl_dataloader)\nMERGE (source)-[:SOURCE_OF]->(dgl_dataset)\nMERGE (source)-[:SOURCE_OF]->(dgl_training)\nMERGE (source)-[:SOURCE_OF]->(dgl_graph)\nMERGE (source)-[:SOURCE_OF]->(dgl_node_feature)\nMERGE (source)-[:SOURCE_OF]->(dgl_graph_level_label)\nMERGE (source)-[:SOURCE_OF]->(dgl_training_loop)\nMERGE (source)-[:SOURCE_OF]->(dgl_what_next)\nMERGE (source)-[:SOURCE_OF]->(dgl_advanced_materials)",
        "article_id": "4079dd24-8583-4a61-b8ec-bf407fae5e3c"
      }
    ],
    "signature": {
      "instructions": "Translate RDF to OpenCypher. Assume entities may already exist in the graph. Include a SOURCE_OF relationship between the article and the entities.",
      "fields": [
        {
          "prefix": "Question:",
          "description": "RDF turtle description of the subjects of a webpage"
        },
        {
          "prefix": "Article Id:",
          "description": "ID of the source article"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Response:",
          "description": "OpenCypher query that creates the grounded entities and relations"
        }
      ]
    },
    "extended_signature": {
      "instructions": "Translate the provided RDF turtle description into OpenCypher queries. Ensure to create nodes and relationships for the entities defined in the RDF, linking them with a SOURCE_OF relationship to the specified article ID. Assume that entities may already exist in the graph, and avoid duplication.",
      "fields": [
        {
          "prefix": "Question:",
          "description": "RDF turtle description of the subjects of a webpage"
        },
        {
          "prefix": "Article Id:",
          "description": "ID of the source article"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "${reasoning}"
        },
        {
          "prefix": "Response:",
          "description": "OpenCypher query that creates the grounded entities and relations"
        }
      ]
    }
  }
}